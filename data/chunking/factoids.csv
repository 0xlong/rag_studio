LAMBADA is Backward Chaining for Automated Reasoning in Natural Language.
Mehran Kazemi is a person.
Najoung Kim is a person.
Deepti Bhatia is a person.
Xin Xu is a person.
Deepak Ramachandran is a person.
Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran are from Google Research.
The email address of Mehran Kazemi is mehrankazemi@google.com.
The email address of Najoung Kim is njkim@google.com.
The email address of Deepti Bhatia is bhatiad@google.com.
The email address of Xin Xu is xxujasime@google.com.
The email address of Deepak Ramachandran is ramachandrand@google.com.
Remarkable progress has been made on automated reasoning with natural text.
Language Models (LMs) are used for automated reasoning with natural text.
Methods such as Chain-of-Thought and Selection-Inference are used for automated reasoning with natural text.
These techniques search for proofs in the forward direction from axioms to the conclusion.
Searching for proofs in the forward direction from axioms to the conclusion suffers from a combinatorial explosion of the search space.
The combinatorial explosion of the search space results in high failure rates for problems requiring longer chains of reasoning.
The classical automated reasoning literature has shown that reasoning in the backward direction is significantly more efficient at proof-finding.
Reasoning in the backward direction is from the intended conclusion to supporting axioms.
Importing this intuition into the LM setting, a Backward Chaining algorithm is developed.
The Backward Chaining algorithm is called LAMBADA.
LAMBADA decomposes reasoning into four sub-modules.
These sub-modules are simply implemented by few-shot prompted LM inference.
LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on challenging logical reasoning datasets.
Accuracy boosts are particularly noticeable when deep and accurate proof chains are required.
Automated reasoning has been a fundamental goal for AI since its early days.
Explicitly provided knowledge is used for automated reasoning.
Logical reasoning is an important building block for automated knowledge discovery.
Logical reasoning holds the key for future advances across various scientific domains.
Tremendous progress has been made towards natural language understanding thanks to pretrained language models (LMs) in recent years.
The performance of these models for logical reasoning still lags behind compared to the advancements in other areas such as reading comprehension and question-answering.
The performance of these models for logical reasoning still lags behind compared to the advancements in question-answering.
The performance of these models for logical reasoning still lags behind compared to the advancements in reading comprehension.
Facts are provided.
Rough and cold that is what they say about Blue Bob.
Eric, who is relatively young, is also pretty big and tends to be cold.
Fred is green and cold too.
For being so cold, it's good Harry can remain nice.
Rules are provided.
Rough, cold people are blue.
Big, kind folks are green ones.
If a person is big, rough, and cold, they are also red.
Most round and cold people are often rough.
Cold, young people are also certain to be rough people.
An individual who is big, red and young is also a nice individual.
The goal is to determine if Eric is nice.
The label is Proved.
Figure 1 shows the search trace of LAMBADA on an example from the ParaRules subset of ProofWriter.
The Sign Agreement and failed Fact Check modules are omitted for brevity.
Many problems benefit from LM scaling.
Scaling has been observed to provide limited benefit for solving complex reasoning problems.
For the Gopher family of LMs, the benefit of scaling for logic-based tasks is significantly worse than for other language tasks.
Finetuning initially seemed to enable logical reasoning in LMs.
Further exploration revealed that finetuned LMs mostly exploit spurious correlations as opposed to learning to reason.
Prompting strategies such as Chain-of-Thought and Scratchpad have contributed to improving performance of LMs on reasoning tasks.
Prompting strategies have been also shown to struggle with proof planning for more complex logical reasoning problems.
One solution to the aforementioned problems is to integrate the strength and reliability of classical AI models in logical reasoning with LMs.
LAMBADA is Backward Chaining for Automated Reasoning in Natural Language.
Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran are authors of the paper.
Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran are affiliated with Google Research.
The email addresses of the authors are: mehrankazemi@google.com, njkim@google.com, bhatiad@google.com, xxujasime@google.com, ramachandrand@google.com.
Remarkable progress has been made on automated reasoning with natural text, by using Language Models (LMs).
Remarkable progress has been made on automated reasoning with natural text, by using methods such as Chain-of-Thought and Selection-Inference.
These techniques search for proofs in the forward direction from axioms to the conclusion.
Searching for proofs in the forward direction suffers from a combinatorial explosion of the search space.
The combinatorial explosion of the search space results in high failure rates for problems requiring longer chains of reasoning.
The classical automated reasoning literature has shown that reasoning in the backward direction is significantly more efficient at proof-finding.
Reasoning in the backward direction is from the intended conclusion to supporting axioms.
Importing this intuition into the LM setting, the authors develop a Backward Chaining algorithm, called LAMBADA.
LAMBADA decomposes reasoning into four sub-modules.
These sub-modules are simply implemented by few-shot prompted LM inference.
LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on challenging logical reasoning datasets.
LAMBADA achieves sizable accuracy boosts particularly when deep and accurate proof chains are required.
Automated reasoning has been a fundamental goal for AI since its early days.
The early days of AI were McCarthy, 1959 and Hewitt, 1969.
Logical reasoning, especially reasoning with unstructured, natural text is an important building block for automated knowledge discovery.
Logical reasoning holds the key for future advances across various scientific domains.
Tremendous progress has been made towards natural language understanding thanks to pretrained language models (LMs).
The performance of these models for logical reasoning still lags behind compared to the advancements in other areas such as reading comprehension and question-answering.
The areas such as reading comprehension and question-answering are areas of advancements.
Figure 1: The search trace of LAMBADA on an example from the ParaRules subset of ProofWriter is shown.
The Sign Agreement and failed Fact Check modules are omitted for brevity.
Many problems benefit from LM scaling.
Scaling has been observed to provide limited benefit for solving complex reasoning problems.
Creswell et al. (2023) observed that for the Gopher family of LMs, the benefit of scaling for logic-based tasks is significantly worse than for other language tasks.
While finetuning initially seemed to enable logical reasoning in LMs, further exploration revealed that finetuned LMs mostly exploit spurious correlations.
The correlation between the number of rules and the label is one of the spurious correlations.
Finetuned LMs are opposed to learning to reason.
Prompting strategies such as Chain-of-Thought and Scratchpad have contributed to improving performance of LMs on reasoning tasks.
Prompting strategies have been shown to struggle with proof planning for more complex logical reasoning problems.
One solution to the aforementioned problems is to integrate the strength and reliability of classical AI models in logical reasoning with LMs.
There are two major approaches to logical reasoning (Poole and Mackworth, 2010).
Poole and Mackworth, 2010, is a reference to a source.
Forward Chaining (FC) is one of the approaches to logical reasoning.
One starts from the facts and rules (“theory”) in Forward Chaining (FC).
One iterates between making new inferences and adding them to the theory in Forward Chaining (FC).
The iteration continues until the goal statement can be proved or disproved in Forward Chaining (FC).
Backward Chaining (BC) is the other approach to logical reasoning.
One starts from the goal in Backward Chaining (BC).
One uses the rules to recursively decompose it into sub-goals in Backward Chaining (BC).
The recursion continues until the sub-goals can be proved or disproved based on the theory in Backward Chaining (BC).
Previous approaches to reasoning with LMs mostly incorporate elements of FC into LMs (Tafjord et al., 2021; Creswell et al., 2023).
Tafjord et al., 2021 is a reference to a source.
Creswell et al., 2023 is a reference to a source.
FC requires selecting a subset of facts and rules from the entire set.
Selecting a subset of facts and rules might be difficult for an LM.
The difficulty is due to it requiring a combinatorial search over a large space.
Moreover, deciding when to halt and declare failure to prove is challenging in FC, as also noted by Creswell et al. (2023).
Creswell et al. (2023) noted that deciding when to halt and declare failure to prove is challenging in FC.
Sometimes, deciding when to halt and declare failure to prove in FC is requiring specialized modules trained on intermediate labels (Creswell and Shanahan, 2022).
Creswell and Shanahan, 2022 is a reference to a source.
The classical automated reasoning literature is heavily weighted towards BC or goal-directed strategies for proof-finding.
In this paper, we show experimentally that BC is better suited for text-based deductive logical reasoning.
BC does not require a combinatorial search for subset selection.
There are more natural halting criteria for BC.
We develop a hybrid LAnguage Model augmented BAckwarD chAining technique (LAMBADA).
BC drives the high-level proof planning in LAMBADA.
The LM performs the textual understanding and individual reasoning steps in LAMBADA.
We conduct experiments with challenging datasets for LM reasoning containing examples expressed in naturalistic text.
The datasets contain proof chains of up to 5 hops in depth.
The datasets contain examples where the goal can neither be proved nor disproved from the provided theory.
We show that LAMBADA achieves substantially higher deductive accuracy.
LAMBADA is considerably more likely to generate valid reasoning chains compared to other techniques.
Other techniques find correct conclusions with spurious proof traces.
LAMBADA is also more query efficient than other LM-based modular reasoning approaches.
Our results strongly indicate that future work on reasoning with LMs should incorporate backward chaining or goal-directed planning strategies.
The deep learning based models that have been developed to solve text-based (logical) reasoning tasks can be categorized as follows (see Huang and Chang 2022 for a recent survey of the literature).
Huang and Chang 2022 is a reference to a source.
Pretraining on Relevant Tasks is a category of deep learning models.
Pretraining an LM on corpora relevant to the target reasoning task can lead to improvements (Hendrycks et al., 2021; Shen et al., 2021).
Hendrycks et al., 2021 is a reference to a source.
Shen et al., 2021 is a reference to a source.
Pretraining is costly especially for larger LMs.
Implicit Reasoning is a category of deep learning models.
These approaches finetune LMs to produce the label directly given the input (Clark et al., 2021; Betz et al., 2021; Saeed et al., 2021; Han et al., 2022).
Clark et al., 2021 is a reference to a source.
Betz et al., 2021 is a reference to a source.
Saeed et al., 2021 is a reference to a source.
Han et al., 2022 is a reference to a source.
Reasoning is expected to happen implicitly in the parameters of the LM.
It has been shown that finetuning LMs on logical reasoning tasks makes them learn spurious correlations (Zhang et al., 2022b; Schlegel et al., 2022).
Zhang et al., 2022b is a reference to a source.
Schlegel et al., 2022 is a reference to a source.
Finetuning LMs on logical reasoning tasks is not robust to multi-hop reasoning (Kassner et al., 2020).
Kassner et al., 2020 is a reference to a source.
Finetuning large LMs is costly especially when the dataset is large.
Finetuning large LMs may introduce distributional shocks to the model (Kazemi et al., 2023).
Kazemi et al., 2023 is a reference to a source.
In this paper, we focus on models that only take in-context examples as supervision.
Explicit Reasoning is a category of deep learning models.
Generating the intermediate reasoning steps such as the chain of reasoning (Wei et al., 2022; Nye et al., 2022; Dalvi et al., 2021; Zelikman et al., 2022; Zhang et al., 2022a) has shown substantial improvement for many reasoning tasks (Suzgun et al., 2022).
Wei et al., 2022 is a reference to a source.
Nye et al., 2022 is a reference to a source.
Dalvi et al., 2021 is a reference to a source.
Zelikman et al., 2022 is a reference to a source.
Zhang et al., 2022a is a reference to a source.
Suzgun et al., 2022 is a reference to a source.
Such chains have been explored both in the forward and the backward directions.
Multiple constrained LMs have been used for logical reasoning (Zhang et al., 2022a).
Gontier et al. (2020) investigated how transformer models perform when trained to perform forward or backward chaining, and drew conclusions about their internal reasoning strategies.
Gontier et al. (2020) is a reference to a source.
We compare against a popular recent prompting strategy, namely Chain-of-Thought (CoT) (Wei et al., 2022), from this category.
Wei et al., 2022 is a reference to a source.
Verifiers is a category of deep learning models.
To improve CoT, some works train a verifier using chain-level labels.
The verifier takes a reasoning chain produced by the model as input.
The verifier judges the quality of the chain.
Some works use a verifier (Cobbe et al., 2021; Shen et al., 2021; Jhamtani and Clark, 2020; Zelikman et al., 2022).
Cobbe et al., 2021 is a reference to a source.
Shen et al., 2021 is a reference to a source.
Jhamtani and Clark, 2020 is a reference to a source.
Zelikman et al., 2022 is a reference to a source.
One can generate multiple reasoning chains by running the algorithm multiple times with different decoding temperatures and use the best chain according to the verifier.
Since LAMBADA also...
generates proofs, verifiers are also applicable to our algorithm.
In this paper, we assume not having access to chain-level labels.
We leave experiments with verifiers as future work.
Length generalization is a concept.
A number of approaches specifically look into whether Language Models can generalize from examples requiring shorter reasoning chains to examples requiring longer chains.
The examples are shown to Language Models either as demonstration or as finetuning data.
Anil et al., 2022 is a reference.
Tafjord et al., 2021 is a reference.
With our model, length generalization comes for free.
The model learns the building blocks of solving the problem.
The building blocks are applied as many times as needed to solve the problem.
Modular Reasoning is a concept.
These approaches break the problem into smaller modules.
These approaches use separate Language Models to solve each module.
Zhou et al., 2022 is a reference.
Khot et al., 2023 is a reference.
Sprague et al., 2022 is a reference.
Zhou et al., 2023 is a reference.
Dua et al., 2022 is a reference.
Wang et al., 2022 is a reference.
Schlag et al., 2023 is a reference.
Language Model-based approaches to logical reasoning typically makes use of a single Language Model module.
For example, in Tafjord et al. (2021), a single Language Model module iteratively and exhaustively infers all conclusions based on the facts and rules.
The goal statement is compared against the final set of conclusions to confirm if it can be proved from the theory.
Since exhaustively deriving all conclusions is computationally expensive, Creswell et al. (2023) consider a more scalable approach.
The conclusions that are derived are informed by the goal.
They iteratively apply two LLM modules.
One module selecting a subset of the facts and rules informed by the goal.
The other module making new inferences based on the selected facts and rules and adding it back to the theory.
In this paper, we compare against the second approach.
Natural Language Inference (NLI) is a concept.
Logical reasoning can also be understood as identifying whether a logical entailment relation holds between two propositions.
The propositions are a premise and a hypothesis.
The premise is the theory and the hypothesis is the statement to be proved.
In this sense, NLI models are also relevant.
Inferences under NLI typically adopt a more relaxed notion of entailment rather than purely logical.
Dagan et al., 2013 is a reference.
Bowman et al., 2015 is a reference.
Williams et al., 2018 is a reference.
3 L AMBADA : Language Model Augmented Backward Chaining is a section title.
We focus on performing automated reasoning over facts.
The facts are natural language assertions.
The facts are such as “Nice people are red”.
The facts are coherent but not necessarily grounded in reality.
A rule is a natural language statement that is either of the form, or can be rewritten in the form, “If P then Q”.
e.g., “Rough, cold people are blue” can be rewritten as “If a person is rough and cold, then they are blue”.
P is called the antecedent of the rule.
Q is called the consequent of the rule.
A theory C consists of facts F = {f1, f2, . . . , fn} and rules R = {r1, r2, . . . , rm}.
We let G represent a goal that we would like to prove or disprove based on the theory.
An example theory with fictional characters and rules is demonstrated in Figure 1.
Based on the theory, one should prove or disprove the goal “Eric is nice”.
3.1 Backward Chaining is a subsection title.
Backward chaining (BC) is a strategy for reasoning.
Backward chaining starts from the goal and recursively breaks the goal into sub-goals based on the rules that can be applied to it, until the sub-goals can be proved or disproved based on the facts or no more rules can be applied to break down the sub-goal further.
Figure 1 shows an example of BC applied to a theory to prove a goal.
Initially, BC verifies if the goal can be proved or disproved based on the facts.
This step is omitted from the figure.
Since none of the facts directly prove or disprove the goal, BC next selects a rule that can be applied to break down the goal into sub-goals.
Whether or not a rule applies to a goal is determined by an operation called unification in logic.
Rule6 has the same consequent as the goal so the operation can be applied.
The other rules have different consequents and it cannot be applied.
Using Rule6, the goal can be broken down into three sub-goals that should be proved for the goal to be proved.
BC then makes recursive calls to prove each sub-goal.
The algorithm continues until either a halting criterion is reached.
The halting criterion is reaching a certain depth in search.
Or a sub-goal can no longer be broken down.
e.g., the left sub-tree under “Eric is rough”.
Or all sub-goals are proved.
e.g., the right sub-tree under “Eric is rough”.
The outcome of BC for a goal is either PROVED, DISPROVED, or UNKNOWN.
e.g., its output for the goal in Figure 1 is PROVED.
Its output for “Fred is not green?” is DISPROVED because it contradicts Fact3.
Its output for “Fred is round?” is UNKNOWN because the theory does not entail or contradict it.
To enable applying BC for text-based reasoning, the authors introduce four LM-based modules.
The four LM-based modules are Fact Check, Rule Selection, Goal Decomposition, and Sign Agreement.
Each module is implemented by showing relevant in-context demonstrations to a pretrained LM.
Details are in Appendix D.3.
The authors describe these modules.
The authors then proceed to the full algorithm.
The Fact Check module is a LM-based module.
Given a set of facts F from the theory and a goal G, the Fact Check module verifies if there exists a fact f ∈ F such that f entails G.
If f entails G, the goal is proved.
If f entails the negation of G, the goal is disproved.
If no such fact can be found, then the truth of G remains unknown.
The authors implement Fact Check with two sub-modules.
The first sub-module selects a fact from the set of facts that is most relevant to the goal.
The second sub-module verifies if the goal can be proved or disproved based on that fact.
Since the first sub-module may fail to identify the best fact on the first try, the selected fact can be removed and the sub-modules can be called again.
If the truth of the goal remained unknown after one try, the sub-modules can be called again.
This process can be repeated multiple times.
In the experiments, the authors call the two sub-modules twice.
The Rule Selection module is a LM-based module.
Given a set of rules R from the theory and a goal G, the Rule Selection module identifies the rules r ∈ R such that the consequent of r unifies with G.
These rules are then used for decomposing the goal into sub-goals.
If no such rule can be identified, then the truth of G remains unknown.
The authors implement Rule Selection with two sub-modules.
The first sub-module identifies the consequent of each rule (independent of the goal).
The second sub-module takes the rule consequents and the goal as input and identifies which one unifies with the goal.
Due to the recursive nature of BC, the Rule Selection module may be invoked multiple times during the proof of a goal.
Since identifying the consequent of each rule is independent of the goal, this sub-module only needs to be called once.
The authors select only one fact because the goals and sub-goals in the datasets the authors work with can be proved/disproved using single facts.
The two modules can be adapted to selected multiple facts if this is not the case.
Algorithm 1 is LAMBADA.
Input of Algorithm 1: Theory C = (F, R), Goal G, Max-Depth D.
Line 1: factCheckResult = FactCheck(G, F).
Line 2: if factCheckResult ≠ UNKNOWN then.
Line 3: return factCheckResult.
Line 4: if D == 0 then.
Line 5: return UNKNOWN.
Line 6: Rs = RuleSelection(G, R).
Line 7: for r ∈ Rerank(Rs) do.
Line 8: G = GoalDecomposition(r, G).
Line 9: if ProveSubgoals(C, G, D) then.
Line 10: if SignAgreement(r, G) then.
Line 11: return PROVED.
Line 12: else.
Line 13: return DISPROVED.
Line 14: return UNKNOWN.
The Goal Decomposition module is a LM-based module.
Given a rule r and a goal G such that the consequent of r unifies with G, the Goal Decomposition module identifies the sub-goals that need to be proved in order for G to be proved or disproved.
The sub-goals are identified based on the antecedent of r.
The Sign Agreement module is a LM-based module.
In the case where the authors succeed in proving the antecedent of r, whether the goal is proved or disproved depends on whether the sign of the goal agrees or disagrees with the sign of the consequent of r.
For instance, in Figure 1, for the goal “Eric is nice.”, since the sign of the goal agrees with the sign of the consequent of Rule6 and the antecedent of the rule is proved, the authors conclude that the goal is proved.
If Rule6 was “[...] is not going to be a nice individual.”, then the sign of the goal would disagree with the sign of the consequent and so the authors would conclude that the goal is disproved.
This motivates the fourth module, Sign Agreement.
Given a rule r and a goal G, the Sign Agreement module verifies if the sign of the consequent of r agrees or disagrees with the sign of the goal or not.
Algorithm 1 provides a high-level description of how the four LM modules described earlier can be integrated with BC to enable text-based logical reasoning.
The function calls corresponding to LM modules are color-coded.
LAMBADA can be understood as a depth-first search algorithm.
Algorithm 2 is named ProveSubgoals.
The input of ProveSubgoals is Theory C = (F, R), Sub-Goals G, and Max-Depth D.
The algorithm iterates through each goal G in G.
result is assigned the value of LAMBADA (C, G, D-1).
If result is not equal to PROVED, then the algorithm returns False (assuming conjunction).
The algorithm returns True.
The search algorithm operates over the facts and the rules.
The search algorithm takes as input a theory C = (F, R), a goal G, and a depth D.
D defines a halting criterion for the algorithm based on the maximum allowed depth for the search.
The search depth is a natural halting criterion corresponding to the maximum number of reasoning hops required for answering questions.
Initially, the algorithm uses the Fact Check module to check if G can be proved or disproved using the facts.
If G can be proved or disproved using the facts, then the algorithm stops and returns the result (PROVED or DISPROVED).
If G cannot be proved or disproved, then the algorithm checks the depth D.
If D = 0, then the algorithm stops and returns UNKNOWN.
UNKNOWN indicates that G could not be proved or disproved.
Otherwise, the algorithm proceeds with applying rules.
The Rule Selection module is used to identify the rules Rs from R whose consequent unifies with G.
Once the setRs is identified, if LAMBADA can start with the rules that have a higher chance of succeeding at (dis)proving the goal, it can save computations and be less error-prone.
Therefore, the algorithm includes a Rerank function in LAMBADA.
Based on the intuition that shorter rules are likely to have fewer sub-goals, the algorithm starts the search from shorter rules and proceeds to longer rules if the shorter ones fail.
Shorter rules have a higher chance of success.
The algorithm leaves more sophisticated ranking strategies as future work.
For each selected rule, the algorithm uses the Goal Decomposition module to decompose G into a set of sub-goals G that need to be proved.
The algorithm checks whether those sub-goals can be proved by making recursive calls to the algorithm (with reduced depth).
If the sub-goals can be proved, then the algorithm uses the Sign Agreement module to check whether the sign of the rule consequent agrees or disagrees with the sign of G.
If the sign agrees, then the algorithm returns PROVED.
Otherwise, the algorithm returns DISPROVED.
If there is no rule for which the sub-goals can be proved, then UNKNOWN is returned.
During a proof, LAMBADA may be called multiple times with the same theory and goal.
In Appendix A, the explanation describes how cycles and redundant computations can be avoided using a cache.
Section 4 is titled Experimental Setup.
The author describes baselines and datasets here.
The author provides further implementation details in Appendix D.
Unless stated otherwise, all experiments are based on the PaLM 540B model (Chowdhery et al., 2022).
Section 4.1 is titled Baselines.
The author compares against the following two baselines.
Chain-of-Thought (CoT) (Wei et al., 2022) is a popular neural approach based on demonstrating chains of inference to the LM within the in-context prompt.
In addition to the few-shot demonstrations in <INPUT>/<LABEL> format in typical in-context learning settings, in CoT, an intermediate explanation for the label is also provided ( <INPUT>/<EXPLANATION>/<LABEL>).
In the work, the explanation corresponds to the proof.
Selection-Inference (SI) (Creswell et al., 2023) is a strong modular reasoning approach based on forward chaining.
SI contains two modules: (1) selection, which, guided by the goal, selects a subset of the facts and rules from which new conclusions can be derived toward proving the goal, and (2) inference, which takes the selected facts and rules and derives a new conclusion.
The two modules are called iteratively, each time producing a single conclusion that is added back to the theory before the next iteration.
The iterations continue until a halting criterion is met (a fixed number of steps in Creswell et al. 2023).
Section 4.2 is titled Datasets.
The author experiments with challenging deductive logical reasoning datasets outlined below.
ProofWriter (Tafjord et al., 2021) is a commonly used synthetic dataset for testing logical reasoning when facts and rules are expressed in naturalistic text.
ProofWriter contains two subsets: an open-world assumption (OWA) subset and a closed-world assumption (CW A) subset.
In the paper, the author uses the OWA subset.
Each example is a (theory, goal) pair.
The label is one of {PROVED, DISPROVED, UNKNOWN}.
UNKNOWN indicates that the goal can neither be proved nor disproved.
The dataset has five parts, each part requiring 0, ≤ 1, ≤ 2, ≤ 3 and ≤ 5 hops of reasoning, respectively.
The author reports two sets of results on this dataset.
Figure 2 presents the prediction accuracy results.
The results are on (a) ProofWriter-PUD (b) ProofWriter-PD, (c) PrOntoQA, and (d) ParaRules datasets.
(e) The proof accuracy of CoT and LAMBADA on ProofWriter (Depth-5) is shown for a set of randomly sampled examples.
The models correctly predicted if the goal can be proved or disproved.
Examples labeled UNKNOWN were removed for compatibility with previous work.
All three labels were used.
Intermediate proof chains from ProofWriter are not used by the models in making predictions.
Due to the cost of inference, the first 1000 examples in the test set were used for both cases.
ProofWriter-PD and ProofWriter-PUD refer to these two subsets.
PrOntoQA (Saparov and He, 2023) is a synthetic dataset.
PrOntoQA was created to analyze the capacity of LM-based approaches for logical reasoning.
Compared to ProofWriter, PrOntoQA has lower natural language diversity and less fact/rule variations.
PrOntoQA has no conjunctions.
The search traces typically contain multiple paths with only one of them leading to the proof.
This enables testing the proof planning of different models.
This dataset has multiple versions.
The fictional characters version of PrOntoQA is one of the hardest versions according to Saparov and He (2023).
Each version of PrOntoQA is divided into different parts depending on the depth of reasoning chains required (1, 3, and 5 hops).
ParaRules (Tafjord et al., 2021) is a version of ProofWriter.
The synthetically generated sentences in the theory were rewritten by crowdworkers to increase diversity and naturalness of the text.
This lets us move beyond evaluating reasoning with templatic expressions.
This is a key limitation of the other datasets.
Each fact in ParaRules may be a combination of several sub-facts.
Examples require proof depths of up to 5.
The label can be PROVED , DISPROVED , or UNKNOWN.
Some minor quality issues were found in ParaRules.
The first 500 examples of the test set were manually verified and fixed.
This set was used for evaluation.
The results are described and LAMBADA and the baselines are compared in detail in the Results section.
5.1 Label Prediction Accuracy describes the results.
The results are reported in Figure 2, (a)–(d).
LAMBADA significantly outperforms the baselines.
LAMBADA outperforms especially on ProofWriter-PUD which contains UNKNOWN labels.
LAMBADA shows a 44% relative improvement compared to CoT and 56% compared to SI on Depth-5.
LAMBADA shows a 37% relative improvement compared to CoT and 113% compared to SI on Depth-5 on the higher depths of PrOntoQA.
LAMBADA shows a 43% relative improvement compared to CoT on the ParaRules dataset.
These results overall show the merit of LAMBADA for logical reasoning.
The reasoning capacity of LAMBADA robustly generalizes to more naturalistic expressions.
This is demonstrated by the high accuracy on ParaRules.
The high accuracy on ParaRules is exactly why.
Due to the low performance of SI on ProofWriter and PrOntoQA and its high number of LM calls, the comparison of LAMBADA against CoT for ParaRules was made.
- The values are 0.30, 0.35, 0.40, 0.45, and 0.50.
- The success rate is 0.53.
- The success rate is 0.47.
- The success rate is 0.34.
- The success rate is 0.31.
- The success rate is 0.31.
- Figure 3 shows the success rate of the k-th inference of SI on PrOntoQA (Depth-5) for different values of k.
- The success rate decreases as k increases.
- The size of the input theory becomes larger as k increases.
- The desired outcome is combining the strengths of an LM and a symbolic reasoning algorithm.
- The results in Figure 2(a) reveal a shortcoming of the CoT approach in dealing with UNKNOWN labels.
- There is no natural chain of thought for the examples whose labels are UNKNOWN, unlike the examples for which the label is PROVED or DISPROVED.
- The performance of CoT is competitive for the ProofWriter-PD dataset.
- The accuracy does not diminish substantially with increasing depth.
- The section investigates the reason for this behavior of CoT.
- 5.2 Proof Accuracy is a section.
- The section aims to understand the reason behind the high accuracy of CoT on higher depths of ProofWriter-PD.
- 50 examples were randomly selected from Depth-5 of the dataset where CoT predicted the label correctly.
- The proof chain was manually verified for the 50 examples.
- The proofs generated by LAMBADA were also manually verified following a similar procedure for comparison.
- The results are reported in Figure 2(e).
- LAMBADA mostly produces correct chains.
- CoT produces correct chains only for 28% of the examples.
- Hallucination is the main source of error.
- Hallucination is the main source of error for 48% of the examples.
- Appendix B.2 provides other prominent failure modes.
- The hallucinated facts and rules mostly resulted in shortcuts to the correct answer.
- This hints at the possibility of spurious correlations in ProofWriter-PD that can be exploited by CoT.
- Appendix B.2, Figure 10 provides examples.
- This result is consistent with previous work showing that when LMs are asked to solve logical reasoning end-to-end, they rely on spurious correlations.
- Zhang et al., 2022b is previous work.
- For modular approaches like SI and LAMBADA, the intermediate modules are impervious to the spurious correlations between the input and the label.
- The intermediate modules do not suffer from this issue.
- 5.3 Forward vs. Backward Chaining is a section.
- SI is based on forward chaining.
- The selection module of SI requires a combinatorial search to find the right subset of facts and rules.
- Appendix C provides details.
- The search space becomes progressively larger in each iteration of the algorithm as new inferences are added to the theory.
- The section aims to verify whether the increase in the search space makes forward chaining progressively harder.
- The success rate of the k-th inference of SI was measured for different values of k on Depth-5 of PrOntoQA.
- Appendix B.3 provides details.
- The results are in Figure 3.
- The success rate indeed decreases in the later inferences of the model.
- The size of the input theory is larger in the later inferences of the model.
- A larger space needs to be searched to find the right combination of facts and rules.
- None of the components in LAMBADA require selecting a subset.
- No combinatorial search is required.
- Appendix C provides more details.
- SI also suffers from inferring redundant facts.
- Figure 4 reports the number of unique inferences from SI for the examples in ProofWriter-PD (Depth-5) where SI incorrectly predicted UNKNOWN.
- The result shows that SI inferences contained no redundant facts only 29% of the time.
- In 7% of the cases, all 5 inferred facts were identical.
- In another 10%, only two unique inferences were made.
- This shows that SI, and maybe more generally forward-chaining approaches, suffer from redundant inference.
- SI also over-predicts DISPROVED in the binary case.
- SI over-predicts UNKNOWN in the three-way classification case.
- SI performs even worse than the majority class for Depth-5 of PrOntoQA which has more PROVED labels than DISPROVED.
- These results, together with Figure 2, show that backward chaining is a better choice compared to forward chaining.
- Backward chaining is the backbone of reasoning in LAMBADA.
- Forward chaining is the backbone in SI.
- Figure 4 reports the number of unique inferences generated by SI for Depth-5 of ProofWriter-PUD when selection and inference modules are called five times.
- 1 unique inference is the number of unique inferences generated by SI for Depth-5 of ProofWriter-PUD in 29% of the time.
- 2 unique inferences is the number of unique inferences generated by SI for Depth-5 of ProofWriter-PUD in 10% of the time.
- 3 unique inferences is the number of unique inferences generated by SI for Depth-5 of ProofWriter-PUD in 20% of the time.
- 4 unique inferences is the number of unique inferences generated by SI for Depth-5 of ProofWriter-PUD in 34% of the time.
- 5 unique inferences is the number of unique inferences generated by SI for Depth-5 of ProofWriter-PUD in 7% of the time.
Figure 5 shows prediction accuracy results.
The prediction accuracy results are on (a) ProofWriter-PUD.
(b) ProofWriter-PD is also included in the prediction accuracy results.
Forward CoT is included in the prediction accuracy results.
Backward CoT is included in the prediction accuracy results.
Figure 5(c) compares the proof accuracy of forward and backward CoT on ProofWriter (Depth-5).
The comparison is for a set of randomly sampled examples.
The models correctly predicted the proof label for the examples.
Fact Check has 1 trial.
Fact Check has 2 trials.
Rule Selection is a module.
Goal Decomposition is a module.
Sign Agreement is a module.
Figure 6 shows ProofWriter (val) performance of modules in LAMBADA in isolation.
The performance is for different LM sizes.
PaLM 8B is a LM size.
PaLM 62B is a LM size.
PaLM 540B is a LM size.
Section 5.4 is titled 'Does Backward CoT Suffice?'
The results may raise the question of whether it is enough to directly incorporate the steps of backward chaining into CoT prompts.
The question is whether modularity (as in LAMBADA) is also needed.
To answer this question, we experiment with a backward version of CoT.
The proofs are written in the backward direction from the goal to the premises in the backward version of CoT.
The label accuracies are presented in Figure 5(a)–(b) for ProofWriter-PUD and ProofWriter-PD.
Their proof accuracy on ProofWriter-PD (Depth-5) is presented in Figure 5(c).
The label accuracy of forward and backward CoT are comparable.
Forward CoT leads to better performance on PUD.
Backward CoT leads to better performance on PD.
For proof accuracy, we see a clear difference between the two versions.
Backward CoT produces substantially lower quality proofs compared to forward chaining.
This result is consistent with the observations of Gontier et al. (2020) for fine-tuned LMs.
The above results show that a modular formulation (as in LAMBADA) is key to successful logical reasoning.
Simply providing CoT in the backward direction does not suffice.
We note that future work can use the traces of our model to finetune (smaller) language models (e.g., Zelikman et al. 2022).
Future work can use the traces as training data in future language models to improve their performance with CoT prompting.
Taking the label and proof accuracy results together, there is also a potential that backward CoT models are more heavily relying on spurious correlations for the PD case.
Backward CoT outperformed CoT in the PD case.
Backward CoT achieves a similar label accuracy as forward CoT.
Backward CoT has a much lower proof accuracy.
Section 5.5 is titled 'Qualitative Analysis'.
In Figure 1, we show the search trace created by LAMBADA for an example from ParaRules.
The answer was predicted correctly in the example.
One can see how backward chaining helps LAMBADA effectively search and create the reasoning chain.
One can see how the LM helps fact checking, rule selection, goal decomposition, and sign agreement checking.
In Appendix B.1, we include an example that has a much larger search trace.
Section 5.6 is titled 'Individual Module Analysis'.
To understand which components in LAMBADA are responsible for the failure cases, we computed the individual accuracy of the four modules described in Section 3.
For this purpose, we created four datasets from the validation set of ProofWriter.
Each dataset measures only the performance of one module in isolation.
See Appendix D.1 for details.
Based on the results of the PaLM 540B model in Figure 6, Rule Selection is the lowest performing module.
Goal Decomposition is the second lowest performing module.
It is possible that the Rule Selection module (partially) fails for some examples.
LAMBADA still arrives at a solution.
- Dataset Depth is 101.
- Dataset Depth is 102.
- The average number of inferences for Depth-0 is 27.79.
- The average number of inferences for Depth-1 is 27.30.
- The average number of inferences for Depth-2 is 57.22.
- The average number of inferences for Depth-3 is 99.45.
- The average number of inferences for Depth-5 is 219.34.
- The average number of inferences for SI is 2.98.
- The average number of inferences for Lambada is 7.26.
- Figure 7 compares LAMBADA and SI.
- Figure 7 compares LAMBADA and SI w.r.t. the average number of inference calls they make per example for different subsets of the ProofWriter-PUD dataset.
- The correct conclusion and proof are considered (e.g., if in Figure 1 the third call to Rule Selection only returned Rule5).
- For Fact Check, when the model is allowed to only select one fact, the accuracy is 0.94.
- When the model is allowed to select two facts, the accuracy is near perfect.
- The Sign Agreement module also shows near-perfect accuracy.
- Section 5.6 repeats the experiment from Section 5.6.
- The experiment from Section 5.6 is repeated with PaLM 62B and 8B to examine the effect of LM scale on LAMBADA.
- According to the results in Figure 6, when PaLM 62B is used, the performance of the Goal Decomposition and Sign Agreement modules remain comparable.
- The performance for the Fact Check and Rule Selection modules drop substantially when using PaLM 62B.
- The second two rely on a one-to-many comparison between the goal and each of the facts/rules which may require a larger model capacity.
- In PaLM 8B, the accuracy for all components drops significantly.
- In some cases, the accuracy for all components is becoming close to random prediction in PaLM 8B.
- The extent to which the higher-level reasoning algorithm breaks the problem into sub-problems should be dependent on the scale and power of the base LMs.
- If smaller LMs are used, then one may need finer-grained problem decomposition (e.g., further decomposing the one-to-many comparisons in the selection module).
- As LMs become larger and stronger in the future, one could rely on them to solve problems with a coarser-grained decomposition of the problem.
- Another advantage of LAMBADA is its efficiency compared to other approaches that require multiple LM inference calls per example such as SI.
- In Figure 7, the average number of LM calls per example is compared for different depths of ProofWriter-PUD.
- LAMBADA requires much fewer calls compared to SI, especially at higher depths.
- For Depth-1, LAMBADA requires 3.8x fewer calls.
- For Depth-5, LAMBADA requires 11.8x fewer calls.
- Section 5.9 analyzes the lexical sensitivity of LAMBADA.
- The test set of ProofWriter-PUD was modified by replacing various lexical items (names, adjectives, and verbs) with novel tokens and the rule templates with novel ones.
- The performance of LAMBADA on the original and the modified test sets was compared using the same few-shot examples.
- The details of the modifications are in Appendix B.5.
- As can be seen in Figure 8, the performance of LAMBADA remains almost unchanged.
- The performance of LAMBADA demonstrates robustness to lexical and templatic variations.
- Figure 8 shows the performance of LAMBADA on ProofWriter-PUD for the original, novel token, and novel template test sets.
- LAMBADA is an algorithm for deductive logical reasoning with natural language.
- LAMBADA combines the capacity of LMs to handle naturalistic text input with the backward chaining algorithm for robust symbolic reasoning.
- LAMBADA achieves significant improvements over competitive approaches on challenging benchmarks, both in terms of label accuracy (predicting if a statement can be proved or disproved based on a theory) and proof accuracy.
- This improvement was also observed in a dataset that expresses the theory in more naturalistic expressions, clearly illustrating the benefit of combining an LM with reasoning modules.
- The query efficiency and lexical robustness of LAMBADA were also demonstrated.
- In this paper, only formal reasoning problems and datasets were experimented with.
- The key insight on the efficacy of backward, goal-directed reasoning with LMs has broader implications.
- The key insight on the efficacy of backward, goal-directed reasoning with LMs can be adapted to other NLP tasks where multi-step inference is required.
The authors identify some limitations and risks with their current work.
The limitations and risks can be addressed in future work.
The current work is mainly applicable to logical entailment problems.
In logical entailment problems, one needs to solve a classification problem.
The classification problem is about whether a goal can be proved, disproved, or neither proved nor disproved based on a theory.
Future work can extend LAMBADA to non-classification cases.
In non-classification cases, one needs to apply logical reasoning to answer questions such as 'What color is Fiona?'.
The current work assumes all the rules are given as input.
The rule set is small enough to be included in the prompt.
Future work can extend LAMBADA to the cases where not all the rules are provided as input.
Part of the knowledge has to come from the LM itself in future work.
Future work can extend LAMBADA to the case where not all the rules can be included in the prompt due to the limitation in the prompt length.
The current work is limited to deductive reasoning with the modus ponens rule.
Future work can expand the applicability of LAMBADA on datasets with other types of rules such as proof by contradiction, disjunction elimination, etc.
The calls made to the LM modules in LAMBADA are dependent on the value from the previous call.
The authors need to wait for the results from one call before they decide what the next call must be.
Making batch calls to the LMs is typically easier and faster.
Future work can find ways to implement LAMBADA with batch LM calls.
LAMBADA is more efficient than SI in terms of the number of inference calls it makes to the LM.
LAMBADA still requires many more calls to the LM compared to approaches such as CoT.
LAMBADA is increasing the required computation and cost.
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur published a paper in 2022.
The paper is titled 'Exploring length generalization in large language models'.
The paper was published in Advances in Neural Information Processing Systems, volume 35, pages 38546–38556.
Gregor Betz, Christian Voigt, and Kyle Richardson published a paper in 2021.
The paper is titled 'Critical thinking for language models'.
The paper was published in Proceedings of the 14th International Conference on Computational Semantics (IWCS), pages 63–75.
The conference was in Groningen, The Netherlands (online).
Association for Computational Linguistics published the paper.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning published a paper in 2015.
The paper is titled 'A large annotated corpus for learning natural language inference'.
The paper was published in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642.
The conference was in Lisbon, Portugal.
Association for Computational Linguistics published the paper.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei published a paper in 2020.
The paper is titled 'Language models are few-shot learners'.
The paper was published in Advances in Neural Information Processing Systems, volume 33, pages 1877–1901.
Curran Associates, Inc. published the paper.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel published a paper in 2022.
The paper is titled 'PaLM: Scaling language modeling with pathways'.
The paper is available on arXiv:2204.02311.
Peter Clark, Oyvind Tafjord, and Kyle Richardson published a paper in 2021.
The paper is titled 'Transformers as soft reasoners over language'.
The paper was published in Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI’20.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman published a paper in 2021.
The paper is titled 'Training verifiers to solve math word problems'.
The paper is available on arXiv:2110.14168.
Antonia Creswell and Murray Shanahan published a paper in 2022.
The paper is titled 'Faithful reasoning using large language models'.
The paper is available on arXiv:2208.14271.
- Antonia Creswell, Murray Shanahan, and Irina Higgins are authors.
- The authors published a work in 2023.
- The work is titled Selection-inference: Exploiting large language models for interpretable logical reasoning.
- The work was published in The Eleventh International Conference on Learning Representations.
- Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto are authors.
- The authors published a work in 2013.
- The work is titled Recognizing textual entailment: Models and applications.
- The work was published in Synthesis Lectures on Human Language Technologies, 6(4):1–220.
- Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark are authors.
- The authors published a work in 2021.
- The work is titled Explaining answers with entailment trees.
- The work was published in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7358–7370, Online and Punta Cana, Dominican Republic.
- The work was published by the Association for Computational Linguistics.
- Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner are authors.
- The authors published a work in 2022.
- The work is titled Successive prompting for decomposing complex questions.
- The work was published in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 1251–1265, Abu Dhabi, United Arab Emirates.
- The work was published by the Association for Computational Linguistics.
- Artur d’Avila Garcez and Luis C Lamb are authors.
- The authors published a work in 2020.
- The work is titled Neurosymbolic ai: the 3rd wave.
- The work is available on arXiv:2012.05876.
- Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Chris Pal are authors.
- The authors published a work in 2020.
- The work is titled Measuring systematic generalization in neural proof generation with transformers.
- The work was published in Advances in Neural Information Processing Systems, volume 33, pages 22231–22242.
- The work was published by Curran Associates, Inc.
- Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et al. are authors.
- The authors published a work in 2022.
- The work is titled FOLIO: Natural language reasoning with first-order logic.
- The work is available on arXiv:2209.00840.
- Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt are authors.
- The authors published a work in 2021.
- The work is titled Measuring mathematical problem solving with the math dataset.
- The work was published in Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1.
- The work was published by Curran.
- Carl Hewitt is an author.
- The author published a work in 1969.
- The work is titled Planner: A language for proving theorems in robots.
- The work was published in Proceedings of the 1st International Joint Conference on Artificial Intelligence, IJCAI’69, page 295–301, San Francisco, CA, USA.
- The work was published by Morgan Kaufmann Publishers Inc.
- Jie Huang and Kevin Chen-Chuan Chang are authors.
- The authors published a work in 2022.
- The work is titled Towards reasoning in large language models: A survey.
- The work is available on arXiv:2212.10403.
- Harsh Jhamtani and Peter Clark are authors.
- The authors published a work in 2020.
- The work is titled Learning to explain: Datasets and models for identifying valid reasoning chains in multihop question-answering.
- The work was published in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 137–150, Online.
- The work was published by the Association for Computational Linguistics.
- Nora Kassner, Benno Krojer, and Hinrich Schütze are authors.
- The authors published a work in 2020.
- The work is titled Are pretrained language models symbolic reasoners over knowledge?.
- The work was published in Proceedings of the 24th Conference on Computational Natural Language Learning, pages 552–564, Online.
- The work was published by the Association for Computational Linguistics.
- Mehran Kazemi, Sid Mittal, and Deepak Ramachandran are authors.
- The authors published a work in 2023.
- The work is titled Understanding finetuning for factual knowledge extraction from language models.
- The work is available on arXiv:2301.11293.
- Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal are authors.
- The authors published a work in 2023.
- The work is titled Decomposed prompting: A modular approach for solving complex tasks.
- The work was published in The Eleventh International Conference on Learning Representations.
- Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang are authors.
- The authors published a work in 2023.
- The work is titled Transformers learn shortcuts to automata.
- The work was published in The Eleventh International Conference on Learning Representations.
- Gary Marcus is an author.
- The author published a work in 2020.
- The work is titled The next decade in AI: four steps towards robust artificial intelligence.
- The work is available on arXiv:2002.06177.
- John McCarthy is an author.
- The author published a work in 1959.
- The work is titled Programs with common sense.
- The work was published in Proceedings of the Teddington Conference on the Mechanization of Thought Processes, pages 75–91, London.
- The work was published by Her Majesty’s Stationary Office.
- Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena are authors.
- The authors published a work in 2022.
- The work is titled Show your work: Scratchpads for intermediate computation with language models.
- The work was published in Deep Learning for Code Workshop.
- David L Poole and Alan K Mackworth are authors.
- The authors published a work in 2010.
- The work is titled Artificial Intelligence: foundations of computational agents.
- The work was published by Cambridge University Press.
- Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, and Zhitao Gong are authors.
Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving wrote 'Scaling language models: Methods, analysis & insights from training Gopher'.
The publication year of 'Scaling language models: Methods, analysis & insights from training Gopher' is 2021.
The publication is available on arXiv with the identifier arXiv:2112.11446.
Mohammed Saeed, Naser Ahmadi, and Paolo Papotti wrote 'RuleBERT: Teaching soft rules to pre-trained language models'.
Preslav Nakov also contributed to the writing of 'RuleBERT: Teaching soft rules to pre-trained language models'.
The publication year of 'RuleBERT: Teaching soft rules to pre-trained language models' is 2021.
'RuleBERT: Teaching soft rules to pre-trained language models' was published in the Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.
The publication was held in Online and Punta Cana, Dominican Republic.
The publication is associated with the Association for Computational Linguistics.
Abulhair Saparov and He He wrote 'Language models are greedy reasoners: A systematic formal analysis of chain-of-thought'.
The publication year of 'Language models are greedy reasoners: A systematic formal analysis of chain-of-thought' is 2023.
'Language models are greedy reasoners: A systematic formal analysis of chain-of-thought' was published in The Eleventh International Conference on Learning Representations.
Imanol Schlag, Sainbayar Sukhbaatar, Asli Celikyilmaz, Wen-tau Yih, Jason Weston, Jürgen Schmidhuber, and Xian Li wrote 'Large language model programs'.
The publication year of 'Large language model programs' is 2023.
The publication is available on arXiv with the identifier arXiv:2305.05364.
Viktor Schlegel, Kamen Pavlov, and Ian Pratt-Hartmann wrote 'Can transformers reason in fragments of natural language?'.
The publication year of 'Can transformers reason in fragments of natural language?' is 2022.
'Can transformers reason in fragments of natural language?' was published in the Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing.
The publication was held in Abu Dhabi, United Arab Emirates.
The publication is associated with the Association for Computational Linguistics.
Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu wrote 'Generate & rank: A multi-task framework for math word problems'.
The publication year of 'Generate & rank: A multi-task framework for math word problems' is 2021.
'Generate & rank: A multi-task framework for math word problems' was published in Findings of the Association for Computational Linguistics: EMNLP 2021.
The publication was held in Punta Cana, Dominican Republic.
The publication is associated with the Association for Computational Linguistics.
Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett wrote 'Natural language deduction with incomplete information'.
The publication year of 'Natural language deduction with incomplete information' is 2022.
'Natural language deduction with incomplete information' was published in the Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing.
The publication was held in Abu Dhabi, United Arab Emirates.
The publication is associated with the Association for Computational Linguistics.
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, and Denny Zhou wrote 'Challenging big-bench tasks and whether chain-of-thought can solve them'.
The publication year of 'Challenging big-bench tasks and whether chain-of-thought can solve them' is 2022.
The publication is available on arXiv with the identifier arXiv:2210.09261.
Oyvind Tafjord, Bhavana Dalvi, and Peter Clark wrote 'ProofWriter: Generating implications, proofs, and abductive statements over natural language'.
The publication year of 'ProofWriter: Generating implications, proofs, and abductive statements over natural language' is 2021.
'ProofWriter: Generating implications, proofs, and abductive statements over natural language' was published in Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021.
The publication was held Online.
The publication is associated with the Association for Computational Linguistics.
Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati wrote 'Large language models still can’t plan (a benchmark for LLMs on planning and reasoning about change)'.
The publication year of 'Large language models still can’t plan (a benchmark for LLMs on planning and reasoning about change)' is 2022.
'Large language models still can’t plan (a benchmark for LLMs on planning and reasoning about change)' was published in the NeurIPS 2022 Foundation Models for Decision Making Workshop.
Boshi Wang, Xiang Deng, and Huan Sun wrote 'Iteratively prompt pre-trained language models for chain of thought'.
The publication year of 'Iteratively prompt pre-trained language models for chain of thought' is 2022.
'Iteratively prompt pre-trained language models for chain of thought' was published in the Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing.
The publication was held in Abu Dhabi, United Arab Emirates.
The publication is associated with the Association for Computational Linguistics.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V. Le, and Denny Zhou wrote 'Chain-of-thought prompting elicits reasoning in large language models'.
The publication year of 'Chain-of-thought prompting elicits reasoning in large language models' is 2022.
'Chain-of-thought prompting elicits reasoning in large language models' was published in Advances in Neural Information Processing Systems, volume 35.
The publication is associated with Curran Associates, Inc.
Adina Williams, Nikita Nangia, and Samuel Bowman wrote 'A broad-coverage challenge corpus for sentence understanding through inference'.
The publication year of 'A broad-coverage challenge corpus for sentence understanding through inference' is 2018.
'A broad-coverage challenge corpus for sentence understanding through inference' was published in the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers).
The publication is associated with the Association for Computational Linguistics.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman wrote 'STaR: Bootstrapping reasoning with reasoning'.
The publication year of 'STaR: Bootstrapping reasoning with reasoning' is 2022.
'STaR: Bootstrapping reasoning with reasoning' was published in Advances in Neural Information Processing Systems, volume 35.
The publication is associated with Curran Associates, Inc.
Hanlin Zhang, Ziyang Li, Jiani Huang, Mayur Naik, and Eric Xing wrote 'Improved logical reasoning of language models via differentiable symbolic programming'.
The publication year of 'Improved logical reasoning of language models via differentiable symbolic programming' is 2022a.
'Improved logical reasoning of language models via differentiable symbolic programming' was published in the First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022.
Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck wrote 'On the paradox of learning to reason from data'.
The publication year of 'On the paradox of learning to reason from data' is 2022b.
The publication is available on arXiv with the identifier arXiv:2205.11502.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi wrote 'Least-to-most prompting enables complex reasoning in large language models'.
The publication year of 'Least-to-most prompting enables complex reasoning in large language models' is 2023.
'Least-to-most prompting enables complex reasoning in large language models' was published in The Eleventh International Conference on Learning Representations.
Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi wrote 'Teaching algorithmic reasoning via in-context learning'.
The publication year of 'Teaching algorithmic reasoning via in-context learning' is 2022.
The publication is available on arXiv with the identifier arXiv:2211.09066.
The text discusses caching and avoiding loops for LAMBADA.
LAMBADA is a recursive algorithm.
During the proof of an example, Algorithm 1 may be called.
Anne is cold.
Anne is kind.
Charlie is nice.
Dave is white.
Dave is young.
Fiona is blue.
Fiona is white.
If Dave is green and Dave is white, then Dave is blue.
If something is green, then it is nice.
If something is blue and cold, then it is green.
If something is white and young, then it is kind.
If something is cold, then it is blue.
All nice, kind things are green.
All kind, cold things are white.
If something is kind and young, then it is cold.
Dave is not green.
Dave is not green.
Fact Check failed.
Rule6 Decompose.
Fact Check failed.
Dave is kind.
Dave is nice.
Fact Check failed.
Rule Selection.
Rule2.
Decompose.
Dave is green.
Fact Check failed.
Rule Selection.
Rule6 Decompose.
Decompose.
Dave is nice.
Dave is kind.
Stop: Cycle Detected.
Rule3 Decompose.
Fact Check failed.
Dave is blue.
Dave is cold.
Fact Check failed.
Rule Selection.
Rule5.
Decompose.
Dave is cold.
Fact Check failed.
Stop: Max Depth Reached.
Rule3.
Dave is blue.
Dave is cold.
Fact Check failed.
Rule Selection.
Rule5.
Decompose.
Dave is cold.
Fact Check failed.
Rule Selection.
Rule8.
Decompose.
Fact Check failed.
Fact Check failed.
Dave is kind.
Dave is young.
Fact Check failed.
Rule Selection.
Rule4.
Decompose.
Fact Check failed.
Fact Check failed.
Dave is white.
Dave is young.
Already Proved.
Already Proved.
Stop: Other branch failed.
Stop: Other branch failed.
Stop: Other branch failed.
Figure 9 is the search trace of LAMBADA on an example from ProofWriter with depth=5 where the answer was predicted correctly.
The sign agreement module has been omitted for brevity.
The modules color-coded with blue represent the calls where the module retrieved the value from the cache instead of calling the LM.
LAMBADA has the same goal multiple times.
For instance, consider the goal “Eric is nice” for the theory in Figure 1.
Applying Rule6 breaks the goal into three sub-goals.
The first sub-goal is “Eric is big” which is proved using the Fact Check module.
For the second sub-goal, Rule3 is used to compose it into three sub-goals, the first of which we have proved before.
Since we have already proved this sub-goal, we can save a Fact Check call if we cache previous results.
The result of a call to LAMBADA can be different depending on the input max depth.
For example, the algorithm may return UNKNOWN when called for the theory and goal in Figure 1 with max depth 0.
The algorithm may return PROVED when called with max depth 3.
Specifically, if we can prove/disprove a goal at depth d, we can conclude that it can be proved/disproved at depths ≥ d as well and we can get the value from the cache.
Moreover, if the algorithm returns UNKNOWN for a goal at depth d, we can conclude that it will also return UNKNOWN at depths < d.
Therefore, if the algorithm is called for a theory and goal at depth d, we also check other depths to see if we have the results for other depths that apply to this case.
Besides having a cache for the entire algorithm that avoids redundant computations when the truth of a goal has been previously computed for a theory, each individual module can also have its own cache as it is possible that the module is called for the same theory and goal.
We show one such example in Figure 9 (to be discussed in Section B).
LAMBADA may sometimes run into loops.
For example, to prove a (sub-)goal “Fiona is round?”, after recursively identifying rules that unify with it and decomposing it into sub-goals, the algorithm may arrive at a point where it needs to prove the “Fiona is round?” sub-goal, which is equivalent to the initial goal.
To avoid such loops, for each path in the proof trace, we keep track of the (sub-)goals that are to be proved and stop further exploring that branch of the search trace when a loop is identified.
In Algorithm 1, for clarity of the algorithm we did not include the caching and loop avoidance operations.
Caching and loop avoidance mainly help with reducing the number of inference calls.
B Additional Results and Analyses.
In this section, we provide some more in-depth qualitative and quantitative analysis of the results from our model and the baselines.
B.1 Qualitative Analysis.
In Figure 9, we provide the search trace of LAMBADA for an example in ProofWriter (Depth-5) for which LAMBADA correctly predicted that the goal is disproved based on the theory.
We deliberately selected an example with a large search trace to demonstrate the various aspects of LAMBADA.
The bald eagle is green.
The bald eagle is young.
The bald eagle sees the dog.
The bear likes the dog.
The bear needs the cow.
The cow needs the dog.
The cow sees the dog.
The dog is blue.
The dog is green.
The dog is young.
The dog needs the bear.
The dog needs the cow.
If someone sees the bald eagle and they are nice then the bald eagle needs the bear.
If someone is nice and young then they need the dog.
If someone likes the cow and the cow needs the dog then the cow is kind.
If someone is young and blue then they like the bear.
If someone is blue and they like the bear then the bear likes the cow.
If someone is green and they need the bear then they need the dog.
If someone sees the bear then they are nice.
If someone is kind then they see the bear.
The bear likes the cow.
The bear sees the mouse.
The lion chases the squirrel.
The lion is blue.
The mouse is big.
If someone likes the mouse and they are blue then they are red.
If someone is blue then they see the mouse.
If the lion sees the squirrel and the lion is blue then the lion chases the mouse.
If someone chases the bear then they see the squirrel.
If someone sees the bear then the bear likes the squirrel.
If someone is young and they see the bear then they chase the mouse.
If someone sees the mouse then they chase the bear.
If someone is blue and they chase the mouse then they are young.
Anne is round.
Anne is young.
Charlie is green.
Charlie is round.
Charlie is young.
Erin is big.
Erin is green.
Erin is round.
Erin is young.
Harry is nice.
Harry is white.
All young, white things are round.
If something is nice and green then it is white.
Cold things are round.
Young, green things are nice.
If something is big and green then it is nice.
White, nice things are young.
All green things are cold.
White, round things are green.
All cold, round things are big.
Charlie is not nice.
Anne is blue.
Anne is nice.
Anne is quiet.
Anne is rough.
Anne is white.
Bob is big.
Charlie is rough.
Erin is big.
Erin is nice.
Erin is young.
Quiet, nice things are white.
If something is rough then it is quiet.
If Bob is white then Bob is young.
If Anne is big and Anne is blue then Anne is rough.
If Bob is rough and Bob is quiet then Bob is nice.
Big things are rough.
If Erin is nice and Erin is young then Erin is big.
Erin is not nice.
LAMBADA starts by calling the Fact Check module on the goal which fails to prove or disprove it.
Rule Selection is called which identifies two rules that can be applied: Rule3 and Rule6.
Since Rule6 is shorter, the reranker ranks it higher.
LAMBADA starts with this rule and calls the Goal Decomposition module which breaks the goal into two sub-goals: Dave is nice and Dave is kind.
Starting with the first sub-goal, Face Check fails on it so Rule Selection is called which selects Rule2 and Goal Decomposition decomposes the sub-goal into Dave is green.
Note that if the cycle checking was smart enough to understand that this sub-goal is the negation of the root goal, we could stop further searching this branch.
We currently only do cycle matching for exact matches so the algorithm continues the search trace.
Fact Check fails again so Rule Selection is called which selects Rule3 and Rule6 again, and since Rule6 is shorter the algorithm continues with that rule.
Goal Decomposition breaks the sub-goal into Dave is nice and Dave is kind.
Considering the first sub-goal, the algorithm identifies a cycle and stops the search.
The second sub-goal is also ignored as there is a conjunction between the sub-goals.
The algorithm then continues with calling Goal Decomposition for Rule3 which breaks the sub-goal into Dave is blue and Dave is cold.
Starting with the first sub-goal, since Fact Check fails the algorithm calls Rule Selection which selects Rule5 and Goal Decomposition breaks the sub-goal into Dave is cold.
Face Check fails on this sub-goal and since the maximum depth is reached, the algorithm stops expanding this branch.
Moreover, the branch for Dave is cold is no longer pursued because there was a conjunction between the sub-goals and one of them failed.
Moving on to the right branch in Figure 9, the algorithm calls the Goal Decomposition module for the goal and Rule3.
Since we have previously computed it, the sub-goals Dave is blue and Dave is cold are returned from the cache.
Fact Check is called on Dave is blue and since it has been computed before, the result (failure) is retrieved from the cache.
The Rule Selection module is called, where the result (Rule5) is again retrieved from the cache.
Goal Decomposition is then called and the sub-goal Dave is cold is
- The information was retrieved from the cache.
- Fact Check fails again.
- Rule Selection selects Rule8.
- Goal Decomposition produces two sub-goals: “Dave is kind.” and “Dave is young.”.
- For “Dave is kind.”, Fact Check fails.
- Rule Selection selects Rule4.
- Goal Decomposition produces two sub-goals: “Dave is white.” and “Dave is young.”.
- For the sub-goal “Dave is white.”, Fact Check succeeds in proving it.
- For the sub-goal “Dave is young.”, Fact Check succeeds in proving it.
- The algorithm then also checks “Dave is young.” for the right branch.
- The algorithm gets the result from the cache for “Dave is young.” because this sub-goal has already been proved.
- The algorithm then checks “Dave is cold.” for the rightmost branch.
- The algorithm gets the result from the cache for “Dave is cold.” because this sub-goal has already been proved.
- The model also calls the Sign Agreement module for rules on the right branch (not shown in the Figure).
- The model finds out that the sign of the rules and the sub-goals agree for all cases, except for the very first rule selected (Rule3).
- The model correctly concludes that the goal is disproved.
- Further Analysis of CoT is about the analysis.
- In Figure 2(e), it was observed that CoT mostly produces wrong proof chains even when the predicted label is correct.
- The analysis was through manually analyzing 50 examples for which CoT predicted the correct label.
- The analysis identified three dominant reasons for the chains being wrong: 1- hallucinating rules or facts, 2- not understanding conjunction, and 3- making invalid derivations.
- In Figure 10, failure examples from each category are shown.
- In the example with a hallucinated rule, CoT relies on a rule “if someone chases the mouse then they see the squirrel”.
- The rule “if someone chases the mouse then they see the squirrel” does not appear in the provided set of rules.
- The rule “if someone chases the mouse then they see the squirrel” cannot even be derived with a combination of the rules.
- The high label accuracy of CoT and its low proof accuracy on ProofWriter-PD hint at the possibility of spurious biases that can be exploited by CoT.
- In 9.2% of the examples which require 1+ reasoning hops, the consequent of one of the rules in the theory is the same as the goal to be proved.
- For 98.9% of these examples the label is PROVED.
- In several of these examples, CoT simply concluded that the goal can be proved in 0 hops based on a hallucinated fact.
- The existence of the word “not” in the goal is highly predictive of the label.
- Goals having “not” are mostly DISPROVED.
- Goals not having “not” are mostly PROVED.
- The PUD case solves the latter issue to a large extent as the label for a good portion of the examples with or without “not” in UNKNOWN.
- The spurious correlations also explain the fluctuations in the CoT performance across different depths, as the performance depends on how much those correlations appear in the few-shot demonstrations.
- For SI and LAMBADA, such spurious correlations between the input and the label cannot be exploited because the intermediate modules are impervious to the correlations between the input and the label.
- Forward Chaining Becomes Progressively More Difficult describes the difficulty.
- Algorithms such as SI that are based on forward chaining require a combinatorial search of the theory to find the right subset of facts and rules in each step of the reasoning.
- The search space becomes progressively larger as the algorithm makes new inferences and those inferences are added back to the theory.
- If the initial size of the theory (i.e. the number of facts plus the number of rules) is |C|, when making the k-th inference the size of the theory is |C| + k − 1.
- As the model produces more inferences, the distance to the goal (in terms of the number of hops remaining between the goal and the facts) should reduce.
- The later inferences should be more accurate.
- The increase in the size of the theory (and hence the size of the search space) may result in lower success rates in the later inferences of the SI model.
- To verify this experimentally, the results of SI on depth-5 of PrOntoQA were further analyzed.
- The subset of examples where the label was PROVED but SI failed to find a proof was extracted.
- These are examples where at least one of the inferences is not on the proof chain.
- As a proxy for measuring the responsibility of the k-th inference of the model for the failure, the percentage of times the k-th inference was on the proof chain was measured.
- The proof chain for each test example is provided as part of the dataset.
- It is possible that, e.g., the first inference is not on the proof chain, but the rest of the inferences are.
- The results are reported in Figure 3 in the main text.
- The results show that the chance of producing inferences that are on the proof chain progressively decreases in the later inferences of the model.
The input predicted label is proved, disproved, and unknown.
The true label is 84, 82, and 331.
The input predicted label is proved, disproved, and unknown.
The true label is 23 and 169.
The true label is 14 and 180.
The true label is 53 and 64.
ProofWriter-PUD (Depth-5) is (a).
The input predicted label is proved, disproved, and unknown.
The true label is 34, 15, and 413.
The true label is 0 and 242.
The true label is 4 and 257.
The true label is 19 and 15.
Selection Inference (b) ProofWriter-PUD (Depth-5) is (b).
The input predicted label is proved, disproved, and unknown.
The true label is 163, 149, and 410.
The true label is 2 and 111.
The true label is 0 and 127.
The true label is 14 and 24.
LAMBADA (c) ProofWriter-PUD (Depth-5) is (c).
The input predicted label is proved and disproved.
The true label is 169 and 110.
The true label is 36 and 67.
PrOntoQA (Depth-5) is (d).
The input predicted label is proved and disproved.
The true label is 6 and 176.
The true label is 209 and 9.
Selection Inference (e) PrOntoQA (Depth-5) is (e).
The input predicted label is proved and disproved.
The true label is 203 and 183.
The true label is 12 and 2.
LAMBADA (f) PrOntoQA (Depth-5) is (f).
The input predicted label is proved, disproved, and unknown.
The true label is 112, 95, and 91.
The true label is 3 and 10.
The true label is 8 and 22.
The true label is 87 and 72.
ParaRules is (g).
The input predicted label is proved, disproved, and unknown.
The true label is 222, 224, and 416.
The true label is 8 and 20.
The true label is 6 and 20.
The true label is 40 and 44.
LAMBADA (h) ParaRules is (h).
The figure 11 is confusion matrices.
The size of the input theory and the search space is larger.
B.4 Confusion Matrices is a section.
They reported the overall model accuracies in the main text.
The confusion matrices help better understand the biases of the model.
Figure 11 reports the confusion matrices for the datasets.
According to the results, they observe that whenever LAMBADA predicts PROVED or DISPROVED, the prediction is mostly correct.
The accuracy is slightly more on cases where the prediction is PROVED than DISPROVED.
This is because DISPROVED cases typically involve negation that makes the reasoning more complex.
There are several examples for which the label is PROVED or DISPROVED, whereas the model predicts UNKNOWN.
CoT and SI also show similar behaviour asLAMBADA on ProofWriter-PUD but with a larger bias toward prediction UNKNOWN.
SI shows a large tendency toward predicting DISPROVED for PrOntoQA.
B.5 Lexical Sensitivity Analysis is a section.
They created a new test for ProofWriter-PUD which contains tokens that do not appear in demonstration examples to analyze the lexical sensitivity of LAMBADA.
They manually created a pool of entity names, animal names, adjectives, and verbs and then made the follow.
```
Depth-0 Depth-1 Depth-2 Depth-3 Depth-5 is a value.
0.0 is a value.
0.2 is a value.
0.4 is a value.
0.6 is a value.
0.8 is a value.
1.0Accuracy is a value.
Original T est Set is a value.
Novel T oken T est Set (v1) is a value.
Novel T oken T est Set (v2) is a value.
(a) is a value.
Depth-0 Depth-1 Depth-2 Depth-3 Depth-5 is a value.
0.0 is a value.
0.2 is a value.
0.4 is a value.
0.6 is a value.
0.8 is a value.
1.0Accuracy is a value.
Original T est Set is a value.
Novel T emplate T est Set (v1) is a value.
Novel T emplate T est Set (v2) (b) is a value.
Figure 12: The performance of LAMBADA on ProofWriter-PUD is shown.
The performance is shown for (a) the original and the novel token test sets.
The performance is shown for (b) the original and the novel template test sets.
The results show that LAMBADA is robust to lexical and template modifications.
The system identified all entity names.
The system mapped each entity name to a randomly selected name from the pool.
The system identified all animals.
The system mapped each animal to a randomly selected animal from the pool.
The system identified all adjectives.
The system mapped each adjective to a randomly selected adjective from the pool.
The system identified all verbs.
The system mapped each verb (except the to be verbs) to a randomly selected verb from the pool.
For example, dog may be mapped to bison in one example.
dog may be mapped to camel in another example.
The system tested the performance of LAMBADA on this modified test set.
The system compared the results to the original test set.
The system analyzed the sensitivity to the templates used for the rules.
The system identified the templates used for the rules in the ProofWriter dataset.
The system replaced each template with another template (previously not appearing in the ProofWriter dataset).
For example, the system changed the template “[X] things are [Y]” to “It is a truth that [X] things are always [Y] as well”.
The system tested the performance of LAMBADA on this modified test set using the same few-shot examples as before.
The system compared the results to the original test set.
The system repeated the aforementioned experiments twice for each analysis.
The system used a different set of tokens/templates each time.
The results in Figure 8 in the main text demonstrate the average accuracy across two runs.
The results for individual runs are presented in Figure 12(a), (b) for the two analyses respectively.
According to the results, the system observe some variations in the total accuracy.
For some depths the performance goes slightly down.
For some depths the performance goes slightly up.
The performance stays in the same ballpark.
The performance shows the robustness of LAMBADA .
Moreover, the system observed that LAMBADA performs significantly better than the baselines tested on the original test set.
The system compared the results on the modified test set with those of the baselines reported in the main text.
The system observed that even on this modified test set, LAMBADA performs significantly better than the baselines tested on the original test set.
C is a value.
Combinatorial Search Issue in is a value.
Forward Chaining is a value.
Consider a simple fictional theory with the following facts:
Anne is cold. is a fact.
Anne is nice and pink. is a fact.
Anne is kind. is a fact.
Anne is green. is a fact.
Anne is big and young. is a fact.
Anne is rough. is a fact.
Anne is round. is a fact.
the following rules are used:
Cold, red people are white. is a rule.
Nice, blue people are white. is a rule.
Kind, green people are white. is a rule.
Cold, round people are white. is a rule.
Big, green people are white. is a rule.
and the goal “Anne is white.” is defined.
An approach based on forward chaining requires selecting a subset of the facts and rules from the theory from which this goal can be proved.
Specifically, the approach needs to select “Anne is cold.”, “Anne is round.”, and Cold, round people are white. from the theory.
Such a selection requires a combinatorial search.
Different combinations of facts and rules should be tested to see which one can lead to proving the goal.
An LM may fail to search this space effectively in a single inference call.
SI uses an approximation to reduce the search space.
SI first makes an inference call to an LM to select one fact/rule.
SI then makes another inference call to select the next fact/rule based on the first one.
SI continues to make inference calls until a halting criterion is met.
```
This reduces the search space from a combinatorial space to a linear space.
The search space is a combinatorial space.
The search space is reduced to a linear space.
The facts/rules are not selected jointly.
The chances of selecting the wrong combinations of facts and rules increase.
Repairing a wrong first choice is not possible.
This leads to low performance.
The low performance is evidenced in the experimental results.
With a backward chaining approach such as LAMBADA, no combinatorial search is required.
With a backward chaining approach such as LAMBADA, no approximations to it is required.
The Rule Selection module verifies each rule independently to see which one is applicable.
The Rule Selection module performs a linear scan.
The Goal Decomposition module breaks goals into sub-goals based on each selected rule independently of the other selected rules.
The Fact Check module verifies the existence of a fact that entails or contradicts the goal with a linear search over the facts.
For the experiments, the PaLM 540B model was used for all the models.
The PaLM 540B model was used for both LAMBADA and the baselines.
Chowdhery et al., 2022 is the author of PaLM 540B model.
The models were served on a 4 × 4 TPU v4 architecture.
The decoding temperature was set to zero.
For testing CoT on PrOntoQA, the same demonstration examples as the original work were used.
The wording was slightly changed by adding conjunctive words such as “Since” and “So” to make the chains have a better flow.
The reason for this modification was that the prompts that have a better flow result in better predictions when working with PaLM.
The prompts have a better flow.
The prompts result in better predictions when working with PaLM.
Figure 13 compares the performance for the original prompts vs. the prompts with the conjunctive words added.
The conjunctive words were added to make the sentences flow better.
The latter slightly underperforms on Depth-1.
The reasoning flow is not as important on Depth-1.
The latter substantially improves the results for higher depths.
The results are substantially improved for higher depths, especially Depth-5.
For ProofWriter, similar few-shot examples were written.
For SI, the same demonstration examples as in the original work for ProofWriter were used.
For PrOntoQA, few-shot examples were written following a similar pattern to those for ProofWriter.
For each dataset depth, specific few-shot examples were used/written.
The examples require at most k hops of reasoning.
The CoT demonstrations also require only k hops of reasoning.
Following the original work, examples with chains up to 3 hops were included for ProofWriter Depth-5.
For running CoT on ProofWriter-PUD, extra few-shot examples were included.
The label is UNKNOWN for the extra few-shot examples.
The goal cannot be proved or disproved with a combination of the facts and the rules for these examples.
For running SI on ProofWriter-PUD, after obtaining the inferences by running SI, the inferences and the goal are given to the Fact Check module.
The Fact Check module decides if the goal can be proved, disproved, or neither.
ProofWriter-PD and PrOntoQA are binary datasets.
LAMBADA makes three-way predictions (PROVED, DISPROVED, and UNKNOWN).
To test LAMBADA on these datasets, similar to SI the UNKNOWN and DISPROVED predictions are combined into one class.
For creating datasets for measuring the performance of individual modules in LAMBADA, the process proceeded as follows.
For Fact Check, 100 examples from the Depth-0 examples were randomly selected.
A model prediction is counted to be correct if it produces the same label as the one specified in the ProofWriter dataset.
For Rule Selection, 100 examples were randomly selected and every rule whose consequent unifies with the goal was manually enumerated.
A model prediction is considered correct if it predicts all such rules correctly.
For Goal Decomposition, 100 rules and goals were randomly selected such that the consequent of the rule unifies with the goal and then the sub-goals were manually written.
A model prediction is considered correct if it predicts all the sub-goals correctly.
For Sign Agreement, the same examples from the Goal Decomposition module were re-used and manually labeled them with respect to their sign agreement/disagreement.
Output: The ParaRules dataset has a high amount of variation in the text., The ParaRules dataset has a high amount of variation in the facts., The ParaRules dataset has a high amount of variation in the rules., The ParaRules dataset makes it a valuable benchmark for evaluating text-based logical reasoning., We also found a few quality issues in the ParaRules dataset., The quality issues were introduced when annotators converted facts and rules into natural language form., We describe some of the main issues that we found and fixed., In some cases where the rule was “X and Y imply Z”, the natural language version of the rule produced by annotators was written as if “X implies Y and Z” or “X implies Y or Z”. , The rule is “X and Y imply Z”, The natural language version of the rule was produced by annotators., The natural language version of the rule was written as if “X implies Y and Z”, The natural language version of the rule was written as if “X implies Y or Z”. , The rule “Cold, nice people are red.” was written in natural language form as “Some cold people can be nice at times, and red at at other times.”., The rule is “Cold, nice people are red.”, The natural language form of the rule is “Some cold people can be nice at times, and red at at other times.”., For such cases, we modified the text to make the antecedents and consequent match the original rule., In some cases, the annotator introduced new antecedents in the rule., The annotator is an annotator., For a rule where the antecedents were “green”, “red” and “rough”, the annotator added another antecedent “naive”., The antecedents are “green”, “red” and “rough”., The annotator added another antecedent “naive”., The rule is about if someone is green and naive., For such cases, we removed the extra antecedents., In some cases, the natural language version of a general rule was written for only a specific entity., The natural language version of a general rule was written for only a specific entity., The rule “Rough, young, green people are very round.” was written as “Tom is a rough, young person to know ...” ., The rule is “Rough, young, green people are very round.”, The text is “Tom is a rough, young person to know ...” ., We removed the specific entities and made the rule generally applicable., For some of the facts, we found that the annotator replaced the name of the entity with a pronoun., The annotator is an annotator., The annotator replaced the name of the entity with a pronoun., “Dave is ...” was annotated as “He is ...”., “Dave is ...” is about Dave., The annotation is “He is ...”., We replaced the pronouns with the original entity name in the theory., We provide an overview of the prompts we used for each of the four components of our model for the ProofWriter dataset., The pseudo-code for the Fact Check module is provided in Algorithm 3., For selecting a fact in Fact Check, our prompt looks like the following:, Algorithm 3 FactCheck is about FactCheck., Algorithm 4 RuleSelection is about RuleSelection., Example 1 is an example., The fact is Fact1., The fact is Fact2., The fact is Factn., The question is <QUESTION>., The inference is that for the question <QUESTION> the most relevant fact is Facti., Example K is an example., The fact is Fact1., The fact is Fact2., The fact is Factm., The question is <QUESTION>., The inference is about verifying if the goal/question can be derived from the selected fact., Example 1 is an example., The fact is <FACT>., The question is <QUESTION>., The inference is that the fact <FACT> [X1] the question <QUESTION> so [X2]., Example K is an example., The fact is <FACT>., The question is <QUESTION>., The inference is about the case where the goal can be proved from the fact., We replace [X1] with “is equivalent to” and [X2] with “so the answer is yes” in the case where the goal can be proved from the fact., We replace [X1]with “is the negation of” and [X2] with “so the answer is no” in the case where the goal can be disproved from the fact., The goal can neither be proved nor disproved.,
The question cannot be inferred from the fact.
We replace [X1] with “is neither equivalent nor the negation of”.
We replace [X2] with “so the question cannot be inferred from the fact”.
The pseudo-code for the Rule Selection module is provided in Algorithm 4.
For finding the implication/consequent of the rules, we use the following prompt:
Example 1 Rule1: <RULE1>, Rule2: <RULE2> ... Rulen: <RULEn>
Inference: Rule1 implies [X1], . . ., Rulen implies [Xn].
Example K Rule1: <RULE1>, Rule2: <RULE2> ... Rulem: <RULEm>
Inference: [Xi]s depend on the consequent of each rule.
For rules such as “Rough, nice people are red.” we write [Xi] as “(is; red)”.
For rules such as “If the cat chases the dog then the cat sees the dog.” we write [Xi] as “(cat; chase; dog)”.
For rule selection based on the implications, we use the following prompt:
Example 1 Rule1 implies <IMLP1>, Rule2 implies <IMPL2>, ..., Rulen implies <IMPLn>
Question: <QUESTION>
Inference: The question is about <IMPLq>: Rule1 <IMPL1> [X1] <IMPLq>, . . ., <IMPLn> [Xn] <IMPLq>.
Example K Rule1 implies <IMLP1>, Rule2 implies <IMPL2>, ..., Rulem implies <IMPLm>
Question: <QUESTION>
Inference: where each [X1]is either “is applicable to“ or “not applicable to“ depending on whether the rule can be applied or not.
For goal decomposition, we use the following prompt:
Example 1 Rule: <Rule>
Question: <QUESTION>
Inference: The question subject is <SUBJq> and the rule premises are <PRM>*, so the question breaks down to <SUBQ>*.
Example K Rule: <RULE>
Question: <QUESTION>
Inference: where <SUBJq>indicates the subject of the question, <PRM>*indicates the premises/antecedents in the rule (the * indicates that there might be multiple premises), and <SUBQ>*indicates the sub-goals.
Finally, for sign agreement, we use the following prompt:
Example 1 Rule: <Rule>
Question: <QUESTION>
Inference: The rule implication <IMLPr> is [Xr], the question <IMPLq> is [Xq], so signs [Xd].
Example K Rule: <RULE>
Question: <QUESTION>
Inference: where <IMLPr> shows the implication of the rule and <IMPLq> indicates the implication of the question.
[Xr] and [Xq] are either “positive“ or “negated“ depending on the sign of the implication.
[Xd] is either “agree“ or “disagree“ depending on whether the signs agree or not.
LAMBADA is a Backward Chaining algorithm for Automated Reasoning in Natural Language.
Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran are the authors of the paper.
Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran are affiliated with Google Research.
The email addresses of the authors are: mehrankazemi@google.com, njkim@google.com, bhatiad@google.com, xxujasime@google.com, and ramachandrand@google.com.
Remarkable progress has been made on automated reasoning with natural text.
Language Models (LMs) and methods such as Chain-of-Thought and Selection-Inference are used for automated reasoning with natural text.
These techniques search for proofs in the forward direction from axioms to the conclusion.
The forward direction search suffers from a combinatorial explosion of the search space.
The forward direction search has high failure rates for problems requiring longer chains of reasoning.
The classical automated reasoning literature has shown that reasoning in the backward direction is significantly more efficient at proof-finding.
The backward direction is from the intended conclusion to supporting axioms.
The authors develop a Backward Chaining algorithm, called LAMBADA, by importing this intuition into the LM setting.
LAMBADA decomposes reasoning into four sub-modules.
These sub-modules are simply implemented by few-shot prompted LM inference.
LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on challenging logical reasoning datasets.
This is particularly true when deep and accurate proof chains are required.
Automated reasoning is the ability to draw valid conclusions from explicitly provided knowledge.
Automated reasoning has been a fundamental goal for AI since its early days.
McCarthy, 1959 and Hewitt, 1969 are related to the early days of AI.
Logical reasoning, especially reasoning with unstructured, natural text is an important building block for automated knowledge discovery.
Logical reasoning holds the key for future advances across various scientific domains.
Tremendous progress has been made towards natural language understanding in recent years thanks to pretrained language models (LMs).
Brown et al., 2020 and Chowdhery et al., 2022, i.a., are related to pretrained language models (LMs).
The performance of these models for logical reasoning still lags behind compared to the advancements in other areas such as reading comprehension and question-answering.
Rae et al., 2021; Creswell et al., 2023; and Valmeekam et al., 2022 are related to the performance of models for logical reasoning.
The facts are: 1.Rough and cold that is what they say about Blue Bob.
The facts are: 2.Eric, who is relatively young, is also pretty big and tends to be cold.
The facts are: 3.Fred is green and cold too.
The facts are: 4.For being so cold, it's good Harry can remain nice.
The rules are: 1.Rough, cold people are blue.
The rules are: 2.Big, kind folks are green ones.
The rules are: 3.If a person is big, rough, and cold, they are also red.
The rules are: 4.Most round and cold people are often rough.
The rules are: 5.Cold, young people are also certain to be rough people.
The rules are: 6.An individual who is big, red and young is also a nice individual.
The goal is: Eric is nice?
The label is: Proved
Figure 1: The search trace of LAMBADA on an example from the ParaRules subset of ProofWriter.
The Sign Agreement and failed Fact Check modules are omitted for brevity.
Many problems benefit from LM scaling.
Scaling has been observed to provide limited benefit for solving complex reasoning problems.
Creswell et al. (2023) observed that for the Gopher family of LMs (Rae et al., 2021), the benefit of scaling for logic-based tasks is significantly worse than for other language tasks.
Finetuning initially seemed to enable logical reasoning in LMs (Clark et al., 2021; Tafjord et al., 2021).
Further exploration revealed that finetuned LMs mostly exploit spurious correlations as opposed to learning to reason (Zhang et al., 2022b; Schlegel et al., 2022; Liu et al., 2023).
Prompting strategies such as Chain-of-Thought (Wei et al., 2022) and Scratchpad (Nye et al., 2022) have contributed to improving performance of LMs on reasoning tasks.
Prompting strategies such as Chain-of-Thought and Scratchpad have been also shown to struggle with proof planning for more complex logical reasoning problems (Saparov and He, 2023).
One solution to the aforementioned problems is to integrate the strength and reliability of classical AI models in logical reasoning with LMs (Garcez and Lamb, 2020; Marcus, 2020).
There are two major approaches to logical reasoning., The two major approaches to logical reasoning are described by Poole and Mackworth (2010)., Forward Chaining (FC) is one of the major approaches to logical reasoning., One starts from the facts and rules (theory) in Forward Chaining (FC)., Forward Chaining (FC) iterates between making new inferences and adding them to the theory., The iterations in Forward Chaining (FC) continue until the goal statement can be proved or disproved., Backward Chaining (BC) is one of the major approaches to logical reasoning., One starts from the goal in Backward Chaining (BC)., Backward Chaining (BC) uses the rules to recursively decompose it into sub-goals., The decomposition in Backward Chaining (BC) continues until the sub-goals can be proved or disproved based on the theory., Previous approaches to reasoning with LMs mostly incorporate elements of FC into LMs., Tafjord et al. (2021) and Creswell et al. (2023) are examples of previous approaches to reasoning with LMs., FC requires selecting a subset of facts and rules from the entire set., Selecting a subset of facts and rules from the entire set might be difficult for an LM., Selecting a subset of facts and rules from the entire set requires a combinatorial search over a large space., Deciding when to halt and declare failure to prove is challenging in FC., Creswell et al. (2023) also noted that deciding when to halt and declare failure to prove is challenging in FC., Sometimes, deciding when to halt and declare failure to prove in FC requires specialized modules trained on intermediate labels., Creswell and Shanahan (2022) is an example of the approach that requires specialized modules trained on intermediate labels., The classical automated reasoning literature is heavily weighted towards BC or goal-directed strategies for proof-finding., In this paper, the authors show experimentally that BC is better suited for text-based deductive logical reasoning., BC does not require a combinatorial search for subset selection., There are more natural halting criteria for BC., The authors develop a hybrid LAnguage Model augmented BAckwarD chAining technique (LAMBADA)., BC drives the high-level proof planning in LAMBADA., The LM performs the textual understanding and individual reasoning steps in LAMBADA., The authors conduct experiments with challenging datasets for LM reasoning., The datasets contain examples expressed in naturalistic text., The datasets contain proof chains of up to 5 hops in depth., The datasets contain examples where the goal can neither be proved nor disproved from the provided theory., LAMBADA achieves substantially higher deductive accuracy., LAMBADA is considerably more likely to generate valid reasoning chains compared to other techniques., Other techniques find correct conclusions with spurious proof traces., LAMBADA is also more query efficient than other LM-based modular reasoning approaches., The results strongly indicate that future work on reasoning with LMs should incorporate backward chaining or goal-directed planning strategies., The deep learning based models that have been developed to solve text-based (logical) reasoning tasks can be categorized as follows., Huang and Chang (2022) provide a recent survey of the literature on deep learning based models., Pretraining on Relevant Tasks is one of the categories., Pretraining an LM on corpora relevant to the target reasoning task can lead to improvements., Hendrycks et al. (2021) and Shen et al. (2021) are examples of the approach of pretraining an LM on corpora., Pretraining is costly especially for larger LMs., Implicit Reasoning is one of the categories., These approaches finetune LMs to produce the label directly given the input., Clark et al. (2021), Betz et al. (2021), Saeed et al. (2021), and Han et al. (2022) are examples of the approach of implicit reasoning., Reasoning is expected to happen implicitly in the parameters of the LM., Finetuning LMs on logical reasoning tasks makes them learn spurious correlations., Zhang et al. (2022b) and Schlegel et al. (2022) are examples of the approach that finetuning LMs on logical reasoning tasks., Finetuning LMs on logical reasoning tasks is not robust to multi-hop reasoning., Kassner et al. (2020) is an example of the approach that is not robust to multi-hop reasoning., Finetuning large LMs is costly especially when the dataset is large., Finetuning large LMs may introduce distributional shocks to the model., Kazemi et al. (2023) is an example of the approach that may introduce distributional shocks to the model., The authors focus on models that only take in-context examples as supervision., Explicit Reasoning is one of the categories., Generating the intermediate reasoning steps has shown substantial improvement for many reasoning tasks., Wei et al. (2022), Nye et al. (2022), Dalvi et al. (2021), and Zelikman et al. (2022), Zhang et al. (2022a) are examples of the approach of generating intermediate reasoning steps., Such chains have been explored both in the forward and the backward directions., Multiple constrained LMs for logical reasoning is an example of the approach of exploring chains., Gontier et al. (2020) investigated how transformer models perform when trained to perform forward or backward chaining., Gontier et al. (2020) drew conclusions about their internal reasoning strategies., The authors compare against a popular recent prompting strategy, namely Chain-of-Thought (CoT)., Wei et al. (2022) proposed Chain-of-Thought (CoT)., Verifiers are one of the approaches to improve CoT., Some works train a verifier using chain-level labels to improve CoT., The verifier takes a reasoning chain produced by the model as input., The verifier judges the quality of the chain., Cobbe et al. (2021), Shen et al. (2021), Jhamtani and Clark (2020), and Zelikman et al. (2022) are examples of approaches that use verifiers., One can generate multiple reasoning chains using the verifier., One can use the best chain according to the verifier., LAMBADA also...
The algorithm generates proofs.
Verifiers are also applicable to our algorithm.
In this paper, the authors assume not having access to chain-level labels.
The authors leave experiments with verifiers as future work.
Length generalization: A number of approaches specifically look into whether LMs can generalize from examples requiring shorter reasoning chains to examples requiring longer chains.
The examples are shown to LMs either as demonstration or as finetuning data.
The examples requiring shorter reasoning chains are shown to LMs.
The examples requiring longer chains are shown to LMs.
Anil et al., 2022 look into length generalization.
Tafjord et al., 2021 look into length generalization.
With our model, length generalization comes for free.
The model learns the building blocks of solving the problem.
The building blocks of solving the problem are applied as many times as needed to solve the problem.
Modular Reasoning: These approaches break the problem into smaller modules.
These approaches use separate LMs to solve each module.
Zhou et al., 2022 use modular reasoning.
Khot et al., 2023 use modular reasoning.
Sprague et al., 2022 use modular reasoning.
Zhou et al., 2023 use modular reasoning.
Dua et al., 2022 use modular reasoning.
Wang et al., 2022 use modular reasoning.
Schlag et al., 2023 use modular reasoning.
LM-based approaches to logical reasoning typically makes use of a single LM module.
For example, in Tafjord et al. (2021), a single LM module iteratively and exhaustively infers all conclusions based on the facts and rules.
The goal statement is compared against the final set of conclusions to confirm if it can be proved from the theory.
Exhaustively deriving all conclusions is computationally expensive.
Creswell et al. (2023) consider a more scalable approach.
The conclusions that are derived are informed by the goal.
They iteratively apply two LLM modules.
One module selects a subset of the facts and rules informed by the goal.
The other module makes new inferences based on the selected facts and rules and adding it back to the theory.
In this paper, the authors compare against the second approach.
Natural Language Inference (NLI): Logical reasoning can also be understood as identifying whether a logical entailment relation holds between two propositions.
The two propositions are premise and hypothesis.
The premise is the theory and the hypothesis is the statement to be proved.
In this sense, NLI models are also relevant.
Inferences under NLI typically adopt a more relaxed notion of entailment rather than purely logical.
Dagan et al., 2013 are relevant to NLI.
Bowman et al., 2015 are relevant to NLI.
Williams et al., 2018 are relevant to NLI.
3 L AMBADA : Language Model Augmented Backward Chaining
The authors focus on performing automated reasoning over facts.
The facts are natural language assertions such as “Nice people are red”.
The facts are coherent but not necessarily grounded in reality.
A rule is a natural language statement that is either of the form, or can be rewritten in the form, “If P then Q”.
“Rough, cold people are blue” can be rewritten as “If a person is rough and cold, then they are blue”.
Pis called the antecedent of the rule.
Q is called the consequent of the rule.
A theory C consists of facts F = {f1, f2, . . . , fn} and rules R = {r1, r2, . . . , rm}.
G represents a goal that the authors would like to prove or disprove based on the theory.
An example theory with fictional characters and rules is demonstrated in Figure 1.
Based on the theory, one should prove or disprove the goal “Eric is nice”.
3.1 Backward Chaining
Backward chaining (BC) is a strategy for reasoning that starts from the goal and recursively breaks the goal into sub-goals based on the rules that can be applied to it.
The algorithm continues until the sub-goals can be proved or disproved based on the facts or no more rules can be applied to break down the sub-goal further.
Figure 1 shows an example of BC applied to a theory to prove a goal.
Initially, BC verifies if the goal can be proved or disproved based on the facts.
This step is omitted from the figure.
Since none of the facts directly prove or disprove the goal, BC next selects a rule that can be applied to break down the goal into sub-goals.
Whether or not a rule applies to a goal is determined by an operation called unification in logic.
Rule6 has the same consequent as the goal so the operation can be applied.
The other rules have different consequents and it cannot be applied.
Using Rule6, the goal can be broken down into three sub-goals that should be proved for the goal to be proved.
BC then makes recursive calls to prove each sub-goal.
The algorithm continues until either a halting criterion is reached.
The halting criterion is reaching a certain depth in search.
The algorithm continues until a sub-goal can no longer be broken down.
The sub-goal is “Eric is rough”.
The algorithm continues until all sub-goals are proved.
The right sub-tree under “Eric is rough”.
The outcome of BC for a goal is either PROVED, DISPROVED, or UNKNOWN.
Its output for the goal in Figure 1 is PROVED.
Its output for “Fred is not green?” is DISPROVED.
The output is because it contradicts Fact3.
Its output for “Fred is round?” is UNKNOWN.
The output is because the theory does not entail or contradict it.
To enable applying BC for text-based reasoning, the system introduces four LM-based modules.
The system introduces four LM-based modules to enable applying BC for text-based reasoning.
The four LM-based modules are: Fact Check, Rule Selection, Goal Decomposition, and Sign Agreement.
Each of the four LM-based modules is implemented by showing relevant in-context demonstrations to a pretrained LM.
Details are in Appendix D.3.
The system describes these modules.
The system then proceeds to the full algorithm.
3.2.1 Fact Check is a section.
The Fact Check module verifies if there exists a fact f from the theory and a goal G.
The Fact Check module verifies if f entails G.
In which case the goal is proved.
The Fact Check module verifies if f entails the negation of G.
In which case the goal is disproved.
If no such fact can be found, then the truth of G remains unknown.
The system implements Fact Check with two sub-modules.
The first sub-module selects a fact from the set of facts that is most relevant to the goal.
The second sub-module verifies if the goal can be proved or disproved based on that fact.
If the first sub-module may fail to identify the best fact on the first try, if the truth of the goal remained unknown after one try, the selected fact can be re-moved.
If the truth of the goal remained unknown after one try, the sub-modules can be called again.
This process can be repeated multiple times.
In the experiments, the system calls the two sub-modules twice.
3.2.2 Rule Selection is a section.
The Rule Selection module identifies the rules r from the theory and a goal G such that the consequent of r unifies with G.
These rules are then used for decomposing the goal into sub-goals.
If no such rule can be identified, then the truth of G remains unknown.
As the system did for Fact Check, the system implements Rule Selection with two sub-modules.
The first sub-module identifies the consequent of each rule (independent of the goal).
The second sub-module takes the rule consequents and the goal as input.
The second sub-module identifies which one unifies with the goal.
Due to the recursive nature of BC, the Rule Selection module may be invoked multiple times during the proof of a goal.
Since identifying the consequent of each rule is independent of the goal, this sub-module only needs to be called once.
Note that the system selects only one fact because the goals and sub-goals in the datasets the system work with can be proved/disproved using single facts.
The two modules can be adapted to selected multiple facts if this is not the case.
Algorithm 1 is LAMBADA.
Input: Theory C = (F, R), Goal G, Max-Depth D.
factCheckResult = FactCheck(G, F).
if factCheckResult ̸= UNKNOWN then return factCheckResult.
if D == 0 then return UNKNOWN.
Rs = RuleSelection(G, R).
for r ∈ Rerank(Rs) do G = GoalDecomposition(r,G).
if ProveSubgoals(C, G, D) then if SignAgreement(r, G) then return PROVED else return DISPROVED.
return UNKNOWN.
3.2.3 Goal Decomposition is a section.
The Goal Decomposition module identifies the sub-goals that need to be proved in order for G to be proved or disproved given a ruler and a goal G such that the consequent of r unifies with G.
The sub-goals are identified based on the antecedent of r.
3.2.4 Sign Agreement is a section.
In the case where the system succeeds in proving the antecedent of r, whether the goal is proved or disproved depends on whether the sign of the goal agrees or disagrees with the sign of the consequent of r.
For instance, in Figure 1, for the goal “Eric is nice.”, since the sign of the goal agrees with the sign of the consequent of Rule6 and the antecedent of the rule is proved, the system concludes that the goal is proved.
If Rule6 was “[...] is not going to be a nice individual.”, then the sign of the goal would disagree with the sign of the consequent and so the system would conclude that the goal is disproved.
This motivates the fourth module, Sign Agreement, described below.
The Sign Agreement module verifies if the sign of the consequent of r agrees or disagrees with the sign of the goal or not given a rule r and a goal G.
3.3 The L AMBADA Algorithm is a section.
Algorithm 1 provides a high-level description of how the four LM modules described earlier can be integrated with BC to enable text-based logical reasoning.
The function calls corresponding to LM modules are color-coded.
LAMBADA can be understood as a depth-first.
Algorithm 2 is named ProveSubgoals.
ProveSubgoals takes input.
The input for ProveSubgoals is Theory C = (F, R).
The input for ProveSubgoals is Sub-Goals G.
The input for ProveSubgoals is Max-Depth D.
The algorithm iterates through G.
For each G in G, result is assigned to LAMBADA(C, G, D-1).
If result is not equal to PROVED, then the algorithm returns False.
The algorithm assumes conjunction.
The algorithm returns True.
The search algorithm operates over the facts and the rules.
The search algorithm takes as input a theory C = (F, R).
The search algorithm takes as input a goal G.
The search algorithm takes as input a depth D.
The depth D defines a halting criterion for the algorithm.
The halting criterion is based on the maximum allowed depth for the search.
The search depth is a natural halting criterion.
The search depth corresponds to the maximum number of reasoning hops required for answering questions.
Initially, the algorithm uses the Fact Check module.
The Fact Check module checks if G can be proved or disproved using the facts.
If G can be proved or disproved using the facts, then the algorithm stops.
The algorithm returns the result (PROVED or DISPROVED).
If G cannot be proved or disproved, then the algorithm checks the depth D.
If D = 0, then the algorithm stops.
The algorithm returns UNKNOWN, indicating that G could not be proved or disproved.
Otherwise, the algorithm proceeds with applying rules.
The Rule Selection module is used to identify the rules Rs from R.
The consequent of the rules Rs unifies with G.
Once the setRs is identified, if LAMBADA can start with the rules that have a higher chance of succeeding at (dis)proving the goal, it can save computations.
If LAMBADA can start with the rules that have a higher chance of succeeding at (dis)proving the goal, it can be less error-prone.
Therefore, we include a Rerank function in LAMBADA.
Based on the intuition that shorter rules are likely to have fewer sub-goals, we start the search from shorter rules and proceed to longer rules if the shorter ones fail.
Shorter rules have a higher chance of success.
We leave more sophisticated ranking strategies as future work.
For each selected rule, the algorithm uses the Goal Decomposition module.
The Goal Decomposition module decomposes G into a set of sub-goals G.
The sub-goals G need to be proved.
The algorithm checks whether those sub-goals can be proved by making recursive calls to the algorithm (with reduced depth).
If the sub-goals can be proved, then the algorithm uses the Sign Agreement module.
The Sign Agreement module checks whether the sign of the rule consequent agrees or disagrees with the sign of G.
If the sign agrees, then the algorithm returns PROVED.
If the sign disagrees, then the algorithm returns DISPROVED.
If there is no rule for which the sub-goals can be proved, then UNKNOWN is returned.
During a proof, LAMBADA may be called multiple times with the same theory and goal.
In Appendix A we explain how cycles and redundant computations can be avoided using a cache.
Section 4 is titled Experimental Setup.
We describe our baselines and datasets here.
We provide further implementation details in Appendix D.
Unless stated otherwise, all experiments are based on the PaLM 540B model.
The PaLM 540B model was created by Chowdhery et al., 2022.
Section 4.1 is titled Baselines.
We compare against the following two baselines.
Chain-of-Thought (CoT) is a popular neural approach.
CoT is based on demonstrating chains of inference to the LM within the in-context prompt.
CoT was created by Wei et al., 2022.
In addition to the few-shot demonstrations in <INPUT>/<LABEL> format in typical in-context learning settings, in CoT, an intermediate explanation for the label is also provided ( <INPUT>/<EXPLANATION>/<LABEL>).
In our work, the explanation corresponds to the proof.
Selection-Inference (SI) is a strong modular reasoning approach.
SI is based on forward chaining.
SI contains two modules.
The modules are (1) selection, which, guided by the goal, selects a subset of the facts and rules from which new conclusions can be derived toward proving the goal, and (2) inference, which takes the selected facts and rules and derives a new conclusion.
SI was created by Creswell et al., 2023.
The two modules are called iteratively.
Each time the modules are called, they produce a single conclusion.
The conclusion is added back to the theory before the next iteration.
The iterations continue until a halting criterion is met.
The halting criterion is a fixed number of steps in Creswell et al. 2023.
Section 4.2 is titled Datasets.
We experiment with challenging deductive logical reasoning datasets.
ProofWriter is a commonly used synthetic dataset for testing logical reasoning.
Facts and rules are expressed in naturalistic text in ProofWriter.
ProofWriter was created by Tafjord et al., 2021.
ProofWriter contains two subsets.
The two subsets are an open-world assumption (OWA) subset and a closed-world assumption (CWA) subset.
In this paper, we use the OWA subset.
Each example is a (theory, goal) pair.
The label is one of {PROVED, DISPROVED, UNKNOWN}.
UNKNOWN indicates that the goal can neither be proved nor disproved.
The dataset has five parts.
Each part requires 0, ≤ 1, ≤ 2, ≤ 3 and ≤ 5 hops of reasoning, respectively.
We report two sets of results on this dataset.
The content describes prediction accuracy results on ProofWriter-PUD, ProofWriter-PD, PrOntoQA, and ParaRules datasets.
The results are reported in Figure 2, (a)–(d).
LAMBADA significantly outperforms the baselines.
LAMBADA especially outperforms the baselines on ProofWriter-PUD which contains UNKNOWN labels.
LAMBADA has a 44% relative improvement compared to CoT on Depth-5 of ProofWriter-PUD.
LAMBADA has a 56% relative improvement compared to SI on Depth-5 of ProofWriter-PUD.
LAMBADA outperforms the baselines on the higher depths of PrOntoQA.
LAMBADA has a 37% relative improvement compared to CoT on Depth-5 of PrOntoQA.
LAMBADA has a 113% relative improvement compared to SI on Depth-5 of PrOntoQA.
LAMBADA outperforms the baselines on the ParaRules dataset.
LAMBADA has a 43% relative improvement compared to CoT on ParaRules.
These results overall show the merit of LAMBADA for logical reasoning.
The reasoning capacity of LAMBADA robustly generalizes to more naturalistic expressions.
The high accuracy on ParaRules demonstrates the generalization of the reasoning capacity of LAMBADA.
ParaRules is a version of ProofWriter.
The synthetically generated sentences in the theory are rewritten by crowdworkers to increase diversity and naturalness of the text in ParaRules.
This lets us move beyond evaluating reasoning with templatic expressions, which is a key limitation of the other datasets.
Each fact in ParaRules may be a combination of several sub-facts.
The examples in ParaRules require proof depths of up to 5.
The label in ParaRules can be PROVED, DISPROVED, or UNKNOWN.
Some minor quality issues were found in ParaRules.
The first 500 examples of the test set of ParaRules were manually verified and fixed.
This set was used for evaluation.
PrOntoQA (Saparov and He, 2023) is a synthetic dataset.
PrOntoQA was created to analyze the capacity of LM-based approaches for logical reasoning.
Compared to ProofWriter, PrOntoQA has lower natural language diversity.
PrOntoQA has less fact/rule variations (e.g., no conjunctions).
The search traces typically contain multiple paths with only one of them leading to the proof in PrOntoQA.
This enables testing the proof planning of different models in PrOntoQA.
PrOntoQA has multiple versions.
The fictional characters version of PrOntoQA is one of the hardest versions according to Saparov and He (2023).
Each version of PrOntoQA is divided into different parts depending on the depth of reasoning chains required (1, 3, and 5 hops).
For both cases, due to the cost of inference, the first 1000 examples in the test set were used.
These two subsets are referred to as ProofWriter-PD and ProofWriter-PUD.
ProofWriter-PUD contains UNKNOWN labels.
Intermediate proof chains from ProofWriter are not used by the models in making predictions.
The proof accuracy of CoT and LAMBADA on ProofWriter (Depth-5) for a set of randomly sampled examples for which the models correctly predicted if the goal can be proved or disproved is (e).
The examples are labeled UNKNOWN removed (for compatibility with previous work).
The examples are with all three labels.
Due to the low performance of SI on ProofWriter and PrOntoQA and its high number of LM calls (see Figure 7), LAMBADA was only compared against CoT for ParaRules.
- The success rate is 0.30 for the 1st k-th inference.
- The success rate is 0.35 for the 2nd k-th inference.
- The success rate is 0.40 for the 3rd k-th inference.
- The success rate is 0.45 for the 4th k-th inference.
- The success rate is 0.50 for the 5th k-th inference.
- The success rate is 0.53 for the k-th inference.
- The success rate is 0.47 for the k-th inference.
- The success rate is 0.34 for the k-th inference.
- The success rate is 0.31 for the k-th inference.
- The success rate is 0.31 for the k-th inference.
- Figure 3 shows the success rate of the k-th inference of SI on PrOntoQA (Depth-5) for different values of k.
- The success rate decreases as k increases.
- The size of the input theory becomes larger as k increases.
- The success rate decreases as the size of the input theory becomes larger.
- The desired outcome is combining the strengths of an LM and a symbolic reasoning algorithm.
- The results in Figure 2(a) reveal a shortcoming of the CoT approach in dealing with UNKNOWN labels.
- There is no natural chain of thought for the examples whose labels are UNKNOWN, unlike the examples for which the label is PROVED or DISPROVED.
- The performance of CoT is competitive for the ProofWriter-PD dataset.
- The accuracy of CoT does not diminish substantially with increasing depth.
- The next section investigates the reason for this behaviour of CoT.
- Section 5.2 is titled Proof Accuracy.
- The reason behind the high accuracy of CoT on higher depths of ProofWriter-PD needs to be understood.
- 50 examples were randomly selected from Depth-5 of the dataset where CoT predicted the label correctly.
- The proof chain was manually verified for the 50 examples.
- The proofs generated by LAMBADA were also manually verified following a similar procedure.
- The results are reported in Figure 2(e).
- LAMBADA mostly produces correct chains.
- CoT produces correct chains only for 28% of the examples.
- Hallucination is the main source of error (48% of the examples; see Appendix B.2 for other prominent failure modes).
- The hallucinated facts and rules mostly resulted in short-cuts to the correct answer.
- This hints at the possibility of spurious correlations in ProofWriter-PD that can be exploited by CoT (see Appendix B.2, Figure 10 for examples).
- This result is consistent with previous work showing that when LMs are asked to solve logical reasoning end-to-end, they rely on spurious correlations (Zhang et al., 2022b).
- For modular approaches like SI and LAMBADA, the intermediate modules are impervious to the spurious correlations between the input and the label.
- For modular approaches like SI and LAMBADA, the intermediate modules do not suffer from this issue.
- Section 5.3 is titled Forward vs. Backward Chaining.
- SI is based on forward chaining.
- The selection module of SI requires a combinatorial search to find the right subset of facts and rules (see Appendix C).
- The search space becomes progressively larger in each iteration of the algorithm as new inferences are added to the theory.
- The increase in the search space makes forward chaining progressively harder.
- The success rate of the k-th inference of SI was measured for different values of k on Depth-5 of PrOntoQA (see Appendix B.3 for details) to verify whether the increase in the search space makes forward chaining progressively harder.
- The success rate indeed decreases in the later inferences of the model.
- The size of the input theory is larger in the later inferences of the model.
- A larger space needs to be searched to find the right combination of facts and rules in the later inferences of the model.
- None of the components in LAMBADA require selecting a subset.
- No combinatorial search is required in LAMBADA (see Appendix C for more details).
- SI also suffers from inferring redundant facts.
- Figure 4 reports the number of unique inferences from SI for the examples in ProofWriter-PD (Depth-5) where SI incorrectly predicted UNKNOWN.
- SI incorrectly predicted UNKNOWN in the examples where a proof exists but SI failed to find it.
- The result shows that SI inferences contained no redundant facts only 29% of the time.
- In 7% of the cases, all 5 inferred facts were identical.
- In another 10%, only two unique inferences were made.
- This shows that SI, and maybe more generally forward-chaining approaches, suffer from redundant inference.
- SI also over-predicts DISPROVED in the binary case.
- SI also over-predicts UNKNOWN in the three-way classification case (see Appendix B.4).
- SI is performing even worse than the majority class for Depth-5 of PrOntoQA which has more PROVED labels than DISPROVED.
- These results, together with Figure 2, show that backward chaining (which is the backbone of reasoning in LAMBADA) is a better choice compared to forward chaining (the backbone in SI).
- Figure 4 shows the number of unique inferences generated by SI for Depth-5 of ProofWriter-PUD when selection and inference modules are called five times.
- 1 unique inference was found in 29% of the cases.
- 2 unique inferences were found in 34% of the cases.
- 3 unique inferences were found in 20% of the cases.
- 4 unique inferences were found in 10% of the cases.
- 5 unique inferences were found in 7% of the cases.
Figure 5 shows prediction accuracy results on (a) ProofWriter-PUD and (b) ProofWriter-PD with forward and backward CoT.
(c) compares the proof accuracy of forward and backward CoT on ProofWriter (Depth-5) for a set of randomly sampled examples for which the models correctly predicted the proof label.
Fact Check has 1 trial.
Fact Check has 2 trials.
Rule Selection is a module.
Goal Decomposition is a module.
Sign Agreement is a module.
PaLM 8B is a model.
PaLM 62B is a model.
PaLM 540B is a model.
Figure 6 shows ProofWriter (val) performance of modules in LAMBADA in isolation, for different LM sizes.
Our results may raise the question of whether it is enough to directly incorporate the steps of backward chaining into CoT prompts, or if modularity (as in LAMBADA ) is also needed.
To answer this question, we experiment with a backward version of CoT where the proofs are written in the backward direction from the goal to the premises.
The label accuracies are presented in Figure 5(a)–(b) for ProofWriter-PUD and ProofWriter-PD.
The label accuracies are presented in Figure 5(c) for the proof accuracy on ProofWriter-PD (Depth-5).
The label accuracy of forward and backward CoT are comparable.
Forward CoT leads to better performance on PUD.
Backward CoT leads to better performance on PD.
For proof accuracy, we see a clear difference between the two versions.
Backward CoT produces substantially lower quality proofs compared to forward chaining.
This result is consistent with the observations of Gontier et al. (2020) for fine-tuned LMs.
A modular formulation (as in LAMBADA ) is key to successful logical reasoning.
Simply providing CoT in the backward direction does not suffice.
Future work can use the traces of our model to finetune (smaller) language models.
Zelikman et al. 2022 is a reference for future work.
Future work can use the traces as training data in future language models to improve their performance with CoT prompting.
Taking the label and proof accuracy results together, there is also a potential that backward CoT models are more heavily relying on spurious correlations for the PD case.
Backward CoT outperformed CoT in the PD case.
Backward CoT achieves a similar label accuracy as forward CoT but with a much lower proof accuracy.
In Figure 1, we show the search trace created by LAMBADA for an example from ParaRules.
The answer was predicted correctly in Figure 1.
Backward chaining helps LAMBADA effectively search and create the reasoning chain.
The LM helps fact checking, rule selection, goal decomposition, and sign agreement checking.
In Appendix B.1, we include an example that has a much larger search trace.
To understand which components in LAMBADA are responsible for the failure cases, we computed the individual accuracy of the four modules described in Section 3.
For this purpose, we created four datasets from the validation set of ProofWriter.
Each dataset measures only the performance of one module in isolation.
(see Appendix D.1 for details).
Based on the results of the PaLM 540B model in Figure 6, Rule Selection is the lowest performing module.
Goal Decomposition is the second lowest performing module.
It is possible that the Rule Selection module (partially) fails for some examples.
LAMBADA still arrives at the correct answer even if the Rule Selection module fails.
- Dataset Depth is 101.
- Dataset Depth is 102.
- The average number of inferences is 27.79.
- The average number of inferences is 27.30.
- The average number of inferences is 57.22.
- The average number of inferences is 99.45.
- The average number of inferences is 219.34.
- The average number of inferences is 2.98.
- The average number of inferences is 7.26.
- The average number of inferences is 10.14.
- The average number of inferences is 12.77.
- The average number of inferences is 18.53.
- SI is a named entity.
- LAMBADA is a named entity.
- Figure 7 compares LAMBADA and SI w.r.t. the average number of inference calls they make per example for different subsets of the ProofWriter-PUD dataset.
- The correct conclusion and proof is a named entity.
- When we allow the model to only select one fact for Fact Check, the accuracy is 0.94.
- When we allow the model to select two facts for Fact Check, the accuracy is near perfect.
- The Sign Agreement module also shows near-perfect accuracy.
- Section 5.7 discusses the role of scale.
- We repeat the experiment from Section 5.6 with PaLM 62B and 8B to examine the effect of LM scale on LAMBADA.
- PaLM 62B is a named entity.
- PaLM 8B is a named entity.
- According to the results in Figure 6, when we use PaLM 62B, the performance of the Goal Decomposition and Sign Agreement modules remain comparable.
- The performance for the Fact Check and Rule Selection modules drop substantially when using PaLM 62B.
- Unlike the first two modules, the second two rely on a one-to-many comparison between the goal and each of the facts/rules which may require a larger model capacity.
- In PaLM 8B, the accuracy for all components drops significantly, in some cases becoming close to random prediction.
- We argue that the extent to which the higher-level reasoning algorithm breaks the problem into sub-problems should be dependent on the scale and power of the base LMs.
- If smaller LMs are used, then one may need finer-grained problem decomposition.
- One may need to further decompose the one-to-many comparisons in the selection module.
- As LMs become larger and stronger in the future, one could rely on them to solve problems with a coarser-grained decomposition of the problem.
- Section 5.8 discusses the number of inference calls.
- Another advantage of LAMBADA is its efficiency compared to other approaches that require multiple LM inference calls per example such as SI.
- In Figure 7, we compare the average number of LM calls per example, for different depths of ProofWriter-PUD.
- LAMBADA requires much fewer calls compared to SI.
- For Depth-1, LAMBADA requires 3.8x fewer calls.
- For Depth-5 it requires 11.8x fewer calls.
- Section 5.9 discusses lexical robustness.
- To analyze the lexical sensitivity of LAMBADA, we modified the test set of ProofWriter-PUD by replacing various lexical items (names, adjectives, and verbs) with novel tokens and the rule templates with novel ones.
- We then compared the performance of LAMBADA on the original and the modified test sets using the same few-shot examples.
- The details of the modifications are in Appendix B.5.
- As can be seen in Figure 8, the performance of LAMBADA remains almost unchanged, demonstrating robustness to lexical and templatic variations.
- Figure 8 shows the performance of LAMBADA on ProofWriter-PUD for the original, novel token, and novel template test sets.
- Section 6 is the conclusion and future directions.
- We developed LAMBADA, an algorithm for deductive logical reasoning with natural language that combines the capacity of LMs to handle naturalistic text input with the backward chaining algorithm for robust symbolic reasoning.
- We showed that LAMBADA achieves significant improvements over competitive approaches on challenging benchmarks, both in terms of label accuracy and proof accuracy.
- The label accuracy is predicting if a statement can be proved or disproved based on a theory.
- This improvement was also observed in a dataset that expresses the theory in more naturalistic expressions, clearly illustrating the benefit of combining an LM with reasoning modules.
- We also demonstrated the query efficiency and lexical robustness of LAMBADA.
- Although in this paper we only experiment with formal reasoning problems and datasets, we believe our key insight on the efficacy of backward, goal-directed reasoning with LMs has broader implications and can be adapted to other NLP tasks where multi-step inference is required.
- The text identifies some limitations and risks with the current work.
- These limitations and risks can be addressed in future work.
- The current work is mainly applicable to logical entailment problems.
- In logical entailment problems, one needs to solve a classification problem.
- The classification problem determines whether a goal can be proved, disproved, or neither proved nor disproved based on a theory.
- Future work can extend LAMBADA to non-classification cases.
- In non-classification cases, one needs to apply logical reasoning to answer questions.
- An example of a non-classification case is answering the question What color is Fiona?.
- The current work assumes all the rules are given as input.
- The rule set is small enough to be included in the prompt in the current work.
- Future work can extend LAMBADA to the cases where not all the rules are provided as input.
- In some cases, part of the knowledge has to come from the LM itself.
- Future work can extend LAMBADA to the case where not all the rules can be included in the prompt due to the limitation in the prompt length.
- The current work is limited to deductive reasoning with the modus ponens rule.
- Future work can expand the applicability of LAMBADA on datasets with other types of rules.
- Examples of other types of rules include proof by contradiction and disjunction elimination.
- The calls made to the LM modules in LAMBADA are dependent on the value from the previous call.
- One needs to wait for the results from one call before deciding what the next call must be.
- Future work can find ways to implement LAMBADA with batch LM calls.
- Making batch calls to the LMs is typically easier and faster.
- LAMBADA is more efficient than SI in terms of the number of inference calls it makes to the LM.
- LAMBADA still requires many more calls to the LM compared to approaches such as CoT.
- This increases the required computation and cost.
- Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur wrote a paper in 2022.
- The paper by Cem Anil et al. is titled Exploring length generalization in large language models.
- The paper by Cem Anil et al. was published in Advances in Neural Information Processing Systems, volume 35, pages 38546–38556.
- Gregor Betz, Christian Voigt, and Kyle Richardson wrote a paper in 2021.
- The paper by Gregor Betz et al. is titled Critical thinking for language models.
- The paper by Gregor Betz et al. was published in the 14th International Conference on Computational Semantics (IWCS), pages 63–75, Groningen, The Netherlands (online).
- The conference was organized by the Association for Computational Linguistics.
- Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning wrote a paper in 2015.
- The paper by Samuel R. Bowman et al. is titled A large annotated corpus for learning natural language inference.
- The paper by Samuel R. Bowman et al. was published in the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal.
- The conference was organized by the Association for Computational Linguistics.
- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei wrote a paper in 2020.
- The paper by Tom Brown et al. is titled Language models are few-shot learners.
- The paper by Tom Brown et al. was published in Advances in Neural Information Processing Systems, volume 33, pages 1877–1901.
- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel wrote a paper in 2022.
- The paper by Aakanksha Chowdhery et al. is titled PaLM: Scaling language modeling with pathways.
- The paper by Aakanksha Chowdhery et al. is available on arXiv:2204.02311.
- Peter Clark, Oyvind Tafjord, and Kyle Richardson wrote a paper in 2021.
- The paper by Peter Clark et al. is titled Transformers as soft reasoners over language.
- The paper by Peter Clark et al. was published in the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI’20.
- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman wrote a paper in 2021.
- The paper by Karl Cobbe et al. is titled Training verifiers to solve math word problems.
- The paper by Karl Cobbe et al. is available on arXiv:2110.14168.
- Antonia Creswell and Murray Shanahan wrote a paper in 2022.
- The paper by Antonia Creswell and Murray Shanahan is titled Faithful reasoning using large language models.
- The paper by Antonia Creswell and Murray Shanahan is available on arXiv:2208.14271.
- Antonia Creswell, Murray Shanahan, and Irina Higgins wrote a paper.
- The paper was published in 2023.
- The paper is titled Selection-inference: Exploiting large language models for interpretable logical reasoning.
- The paper was published in The Eleventh International Conference on Learning Representations.
- Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto wrote a paper.
- The paper was published in 2013.
- The paper is titled Recognizing textual entailment: Models and applications.
- The paper was published in Synthesis Lectures on Human Language Technologies, 6(4):1–220.
- Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark wrote a paper.
- The paper was published in 2021.
- The paper is titled Explaining answers with entailment trees.
- The paper was published in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7358–7370, Online and Punta Cana, Dominican Republic.
- The publisher of the paper is Association for Computational Linguistics.
- Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner wrote a paper.
- The paper was published in 2022.
- The paper is titled Successive prompting for decomposing complex questions.
- The paper was published in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1251–1265, Abu Dhabi, United Arab Emirates.
- The publisher of the paper is Association for Computational Linguistics.
- Artur d’Avila Garcez and Luis C Lamb wrote a paper.
- The paper was published in 2020.
- The paper is titled Neurosymbolic ai: the 3rd wave.
- The paper was published in arXiv:2012.05876.
- Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Chris Pal wrote a paper.
- The paper was published in 2020.
- The paper is titled Measuring systematic generalization in neural proof generation with transformers.
- The paper was published in Advances in Neural Information Processing Systems, volume 33, pages 22231–22242.
- The publisher of the paper is Curran Associates, Inc.
- Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et al. wrote a paper.
- The paper was published in 2022.
- The paper is titled FOLIO: Natural language reasoning with first-order logic.
- The paper was published in arXiv:2209.00840.
- Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt wrote a paper.
- The paper was published in 2021.
- The paper is titled Measuring mathematical problem solving with the math dataset.
- The paper was published in Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1.
- The publisher of the paper is Curran.
- Carl Hewitt wrote a paper.
- The paper was published in 1969.
- The paper is titled Planner: A language for proving theorems in robots.
- The paper was published in Proceedings of the 1st International Joint Conference on Artificial Intelligence, IJCAI’69, page 295–301, San Francisco, CA, USA.
- The publisher of the paper is Morgan Kaufmann Publishers Inc.
- Jie Huang and Kevin Chen-Chuan Chang wrote a paper.
- The paper was published in 2022.
- The paper is titled Towards reasoning in large language models: A survey.
- The paper was published in arXiv:2212.10403.
- Harsh Jhamtani and Peter Clark wrote a paper.
- The paper was published in 2020.
- The paper is titled Learning to explain: Datasets and models for identifying valid reasoning chains in multihop question-answering.
- The paper was published in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 137–150, Online.
- The publisher of the paper is Association for Computational Linguistics.
- Nora Kassner, Benno Krojer, and Hinrich Schütze wrote a paper.
- The paper was published in 2020.
- The paper is titled Are pretrained language models symbolic reasoners over knowledge?.
- The paper was published in Proceedings of the 24th Conference on Computational Natural Language Learning, pages 552–564, Online.
- The publisher of the paper is Association for Computational Linguistics.
- Mehran Kazemi, Sid Mittal, and Deepak Ramachandran wrote a paper.
- The paper was published in 2023.
- The paper is titled Understanding finetuning for factual knowledge extraction from language models.
- The paper was published in arXiv:2301.11293.
- Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal wrote a paper.
- The paper was published in 2023.
- The paper is titled Decomposed prompting: A modular approach for solving complex tasks.
- The paper was published in The Eleventh International Conference on Learning Representations.
- Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang wrote a paper.
- The paper was published in 2023.
- The paper is titled Transformers learn shortcuts to automata.
- The paper was published in The Eleventh International Conference on Learning Representations.
- Gary Marcus wrote a paper.
- The paper was published in 2020.
- The paper is titled The next decade in AI: four steps towards robust artificial intelligence.
- The paper was published in arXiv:2002.06177.
- John McCarthy wrote a paper.
- The paper was published in 1959.
- The paper is titled Programs with common sense.
- The paper was published in Proceedings of the Teddington Conference on the Mechanization of Thought Processes, pages 75–91, London.
- The publisher of the paper is Her Majesty’s Stationary Office.
- Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena wrote a paper.
- The paper was published in 2022.
- The paper is titled Show your work: Scratchpads for intermediate computation with language models.
- The paper was published in Deep Learning for Code Workshop.
- David L Poole and Alan K Mackworth wrote a book.
- The book was published in 2010.
- The book is titled Artificial Intelligence: foundations of computational agents.
- The publisher of the book is Cambridge University Press.
- Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, and Zhitao Gong wrote a paper.
Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving wrote a paper in 2021., The paper is titled 'Scaling language models: Methods, analysis & insights from training Gopher.', The paper is available on arXiv:2112.11446.
Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti wrote a paper in 2021., The paper is titled 'RuleBERT: Teaching soft rules to pre-trained language models.', The paper was published in the Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1460–1476., The conference took place Online and in Punta Cana, Dominican Republic., The Association for Computational Linguistics published the proceedings.
Abulhair Saparov and He He wrote a paper in 2023., The paper is titled 'Language models are greedy reasoners: A systematic formal analysis of chain-of-thought.', The paper was published in The Eleventh International Conference on Learning Representations.
Imanol Schlag, Sainbayar Sukhbaatar, Asli Celikyilmaz, Wen-tau Yih, Jason Weston, Jürgen Schmidhuber, and Xian Li wrote a paper in 2023., The paper is titled 'Large language model programs.', The paper is an arXiv preprint arXiv:2305.05364.
Viktor Schlegel, Kamen Pavlov, and Ian Pratt-Hartmann wrote a paper in 2022., The paper is titled 'Can transformers reason in fragments of natural language?', The paper was published in the Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11184–11199., The conference took place in Abu Dhabi, United Arab Emirates., The Association for Computational Linguistics published the proceedings.
Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu wrote a paper in 2021., The paper is titled 'Generate & rank: A multi-task framework for math word problems.', The paper was published in Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2269–2279., The conference took place in Punta Cana, Dominican Republic., The Association for Computational Linguistics published the findings.
Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett wrote a paper in 2022., The paper is titled 'Natural language deduction with incomplete information.', The paper was published in the Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8230–8258., The conference took place in Abu Dhabi, United Arab Emirates., The Association for Computational Linguistics published the proceedings.
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. wrote a paper in 2022., The paper is titled 'Challenging big-bench tasks and whether chain-of-thought can solve them.', The paper is available on arXiv:2210.09261.
Oyvind Tafjord, Bhavana Dalvi, and Peter Clark wrote a paper in 2021., The paper is titled 'ProofWriter: Generating implications, proofs, and abductive statements over natural language.', The paper was published in Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621–3634., The conference took place Online., The Association for Computational Linguistics published the findings.
Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati wrote a paper in 2022., The paper is titled 'Large language models still can’t plan (a benchmark for LLMs on planning and reasoning about change).', The paper was published in NeurIPS 2022 Foundation Models for Decision Making Workshop.
Boshi Wang, Xiang Deng, and Huan Sun wrote a paper in 2022., The paper is titled 'Iteratively prompt pre-trained language models for chain of thought.', The paper was published in the Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2714–2730., The conference took place in Abu Dhabi, United Arab Emirates., The Association for Computational Linguistics published the proceedings.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V . Le, and Denny Zhou wrote a paper in 2022., The paper is titled 'Chain-of-thought prompting elicits reasoning in large language models.', The paper was published in Advances in Neural Information Processing Systems, volume 35, pages 24824–24837., Curran Associates, Inc. published the paper.
Adina Williams, Nikita Nangia, and Samuel Bowman wrote a paper in 2018., The paper is titled 'A broad-coverage challenge corpus for sentence understanding through inference.', The paper was published in the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122., The Association for Computational Linguistics published the paper.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman wrote a paper in 2022., The paper is titled 'STaR: Bootstrapping reasoning with reasoning.', The paper was published in Advances in Neural Information Processing Systems , volume 35, pages 15476–15488., Curran Associates, Inc. published the paper.
Hanlin Zhang, Ziyang Li, Jiani Huang, Mayur Naik, and Eric Xing wrote a paper in 2022a., The paper is titled 'Improved logical reasoning of language models via differentiable symbolic programming.', The paper was published in First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022.
Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck wrote a paper in 2022b., The paper is titled 'On the paradox of learning to reason from data.', The paper is available on arXiv:2205.11502.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi wrote a paper in 2023., The paper is titled 'Least-to-most prompting enables complex reasoning in large language models.', The paper was published in The Eleventh International Conference on Learning Representations.
Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi wrote a paper in 2022., The paper is titled 'Teaching algorithmic reasoning via in-context learning.', The paper is available on arXiv:2211.09066.
The section is about A Caching and Avoiding Loops for LAMBADA.
LAMBADA is a recursive algorithm.
During the proof of an example, Algorithm 1 may be called.
Anne is cold.
Anne is kind.
Charlie is nice.
Dave is white.
Dave is young.
Fiona is blue.
Fiona is white.
If Dave is green and Dave is white, then Dave is blue.
If something is green, then it is nice.
If something is blue and cold, then it is green.
If something is white and young, then it is kind.
If something is cold, then it is blue.
All nice, kind things are green.
All kind, cold things are white.
If something is kind and young, then it is cold.
Dave is not green.
Dave is not green.
The Fact Check module failed.
Rule6 is selected.
The Fact Check module failed.
Dave is kind.
Dave is nice.
The Fact Check module failed.
Rule2 is selected.
Dave is green.
The Fact Check module failed.
Rule6 is selected.
Dave is nice.
Dave is kind.
A cycle is detected.
Rule3 is selected.
The Fact Check module failed.
Dave is blue.
Dave is cold.
The Fact Check module failed.
Rule5 is selected.
Dave is cold.
The Fact Check module failed.
The max depth is reached.
Rule3 is selected.
Dave is blue.
Dave is cold.
The Fact Check module failed.
Rule5 is selected.
Dave is cold.
The Fact Check module failed.
Rule8 is selected.
Dave is kind.
Dave is young.
The Fact Check module failed.
Rule4 is selected.
Dave is white.
Dave is young.
The goal is already proved.
The goal is already proved.
Other branch failed.
Other branch failed.
Other branch failed.
Figure 9 is the search trace of LAMBADA on an example from ProofWriter with depth=5 where the answer was predicted correctly.
The sign agreement module has been omitted for brevity.
The modules color-coded with blue represent the calls where the module retrieved the value from the cache instead of calling the LM.
LAMBADA may run with the same goal multiple times.
Consider the goal “Eric is nice” for the theory in Figure 1.
Applying Rule6 breaks the goal into three sub-goals.
The first sub-goal is “Eric is big”.
“Eric is big” is proved using the Fact Check module.
For the second sub-goal, Rule3 is used to compose it into three sub-goals.
The first sub-goal has been proved before.
Since we have already proved this sub-goal, we can save a Fact Check call if we cache previous results.
The result of a call to LAMBADA can be different depending on the input max depth.
The algorithm may return UNKNOWN when called for the theory and goal in Figure 1 with max depth 0.
The algorithm may return PROVED when called with max depth 3.
If we can prove/disprove a goal at depth d, we can conclude that it can be proved/disproved at depths ≥ d as well.
We can get the value from the cache.
If the algorithm returns UNKNOWN for a goal at depth d, we can conclude that it will also return UNKNOWN at depths < d.
If the algorithm is called for a theory and goal at depth d, we also check other depths to see if we have the results for other depths that apply to this case.
Besides having a cache for the entire algorithm that avoids redundant computations when the truth of a goal has been previously computed for a theory, each individual module can also have its own cache.
It is possible that the module is called for the same theory and goal.
We show one such example in Figure 9 (to be discussed in Section B).
LAMBADA may sometimes run into loops.
For example, to prove a (sub-)goal “Fiona is round?” after recursively identifying rules that unify with it and decomposing it into sub-goals, the algorithm may arrive at a point where it needs to prove the “Fiona is round?” sub-goal, which is equivalent to the initial goal.
To avoid such loops, for each path in the proof trace, we keep track of the (sub-)goals that are to be proved.
We stop further exploring that branch of the search trace when a loop is identified.
In Algorithm 1, for clarity of the algorithm we did not include the caching and loop avoidance operations.
Caching and loop avoidance mainly help with reducing the number of inference calls.
In Section B, we provide some more in-depth qualitative and quantitative analysis of the results from our model and the baselines.
In Figure 9, we provide the search trace of LAMBADA for an example in ProofWriter (Depth-5) for which LAMBADA correctly predicted that the goal is disproved based on the theory.
We deliberately selected an example with a large search trace to demonstrate the various aspects of LAMBADA.
The bald eagle is green.
The bald eagle is young.
The bald eagle sees the dog.
The bear likes the dog.
The bear needs the cow.
The cow needs the dog.
The cow sees the dog.
The dog is blue.
The dog is green.
The dog is young.
The dog needs the bear.
The dog needs the cow.
If someone sees the bald eagle and they are nice then the bald eagle needs the bear.
If someone is nice and young then they need the dog.
If someone likes the cow and the cow needs the dog then the cow is kind.
If someone is young and blue then they like the bear.
If someone is blue and they like the bear then the bear likes the cow.
If someone is green and they need the bear then they need the dog.
If someone sees the bear then they are nice.
If someone is kind then they see the bear.
The bear likes the cow.
The bear sees the mouse.
The lion chases the squirrel.
The lion is blue.
The mouse is big.
If someone likes the mouse and they are blue then they are red.
If someone is blue then they see the mouse.
If the lion sees the squirrel and the lion is blue then the lion chases the mouse.
If someone chases the bear then they see the squirrel.
If someone sees the bear then the bear likes the squirrel.
If someone is young and they see the bear then they chase the mouse.
If someone sees the mouse then they chase the bear.
If someone is blue and they chase the mouse then they are young.
Anne is round.
Anne is young.
Charlie is green.
Charlie is round.
Charlie is young.
Erin is big.
Erin is green.
Erin is round.
Erin is young.
Harry is nice.
Harry is white.
All young, white things are round.
If something is nice and green then it is white.
Cold things are round.
Young, green things are nice.
If something is big and green then it is nice.
White, nice things are young.
All green things are cold.
White, round things are green.
All cold, round things are big.
Charlie is not nice.
Anne is blue.
Anne is nice.
Anne is quiet.
Anne is rough.
Anne is white.
Bob is big.
Charlie is rough.
Erin is big.
Erin is nice.
Erin is young.
Quiet, nice things are white.
If something is rough then it is quiet.
If Bob is white then Bob is young.
If Anne is big and Anne is blue then Anne is rough.
If Bob is rough and Bob is quiet then Bob is nice.
Big things are rough.
If Erin is nice and Erin is young then Erin is big.
Erin is not nice.
LAMBADA starts by calling the Fact Check module on the goal which fails to prove or disprove it.
Rule Selection is called which identifies two rules that can be applied: Rule3 and Rule6.
Since Rule6 is shorter, the reranker ranks it higher.
LAMBADA starts with this rule and calls the Goal Decomposition module which breaks the goal into two sub-goals: “Dave is nice.” and “Dave is kind.”.
Starting with the first sub-goal, Face Check fails on it so Rule Selection is called which selects Rule2.
Goal Decomposition decomposes the sub-goal into “Dave is green.”.
If the cycle checking was smart enough to understand that this sub-goal is the negation of the root goal, we could stop further searching this branch.
We currently only do cycle matching for exact matches so the algorithm continues the search trace.
Fact Check fails again so Rule Selection is called which selects Rule3 and Rule6 again.
Since Rule6 is shorter the algorithm continues with that rule.
Goal Decomposition breaks the sub-goal into “Dave is nice.” and “Dave is kind.”.
Considering the first sub-goal, the algorithm identifies a cycle and stops the search.
The second sub-goal is also ignored as there is a conjunction between the sub-goals.
The algorithm then continues with calling Goal Decomposition for Rule3 which breaks the sub-goal into “Dave is blue.” and “Dave is cold.”.
Starting with the first sub-goal, since Fact Check fails the algorithm calls Rule Selection which selects Rule5.
Goal Decomposition breaks the sub-goal into “Dave is cold.”.
Face Check fails on this sub-goal.
Since the maximum depth is reached, the algorithm stops expanding this branch.
The branch for “Dave is cold.” is no longer pursued because there was a conjunction between the sub-goals and one of them failed.
Moving on to the right branch in Figure 9, the algorithm calls the Goal Decomposition module for the goal and Rule3.
Since we have previously computed it, the sub-goals “Dave is blue.” and “Dave is cold.” are returned from the cache.
Fact Check is called on “Dave is blue.” and since it has been computed before, the result (failure) is retrieved from the cache.
The Rule Selection module is called, where the result (Rule5) is again retrieved from the cache.
Goal Decomposition is then called and the sub-goal “Dave is cold.” is
- The information was retrieved from the cache.
- Fact Check fails again.
- Rule Selection selects Rule8.
- Goal Decomposition produces two sub-goals: “Dave is kind.” and “Dave is young.”.
- For “Dave is kind.”, Fact Check fails.
- Rule Selection selects Rule4.
- Goal Decomposition produces two sub-goals: “Dave is white.” and “Dave is young.”.
- For both of these sub-goals, Fact Check succeeds in proving them.
- The algorithm then also checks “Dave is young.” for the right branch.
- This sub-goal has already been proved.
- The algorithm just gets the result from the cache.
- The algorithm then checks “Dave is cold.” for the rightmost branch.
- This sub-goal has already been proved.
- The algorithm just gets the result from the cache.
- The model also calls the Sign Agreement module for rules on the right branch.
- The Sign Agreement module is not shown in the Figure.
- The model finds out that the sign of the rules and the sub-goals agree for all cases, except for the very first rule selected (Rule3).
- The model correctly concludes that the goal is disproved.
- CoT mostly produces wrong proof chains even when the predicted label is correct.
- The analysis was performed in Figure 2(e).
- The analysis was performed on 50 examples for which CoT predicted the correct label.
- Three dominant reasons for the chains being wrong were identified.
- The reasons are: 1- hallucinating rules or facts, 2- not understanding conjunction, and 3- making invalid derivations.
- In Figure 10, failure examples from each category are shown.
- In the example with a hallucinated rule, CoT relies on a rule “if someone chases the mouse then they see the squirrel”.
- The rule “if someone chases the mouse then they see the squirrel” does not appear in the provided set of rules.
- The rule “if someone chases the mouse then they see the squirrel” cannot even be derived with a combination of the rules.
- The high label accuracy of CoT and its low proof accuracy on ProofWriter-PD hint at the possibility of spurious biases that can be exploited by CoT.
- For example, in 9.2% of the examples which require 1+ reasoning hops, the consequent of one of the rules in the theory is the same as the goal to be proved.
- For 98.9% of these examples the label is PROVED.
- In several of these examples, CoT simply concluded that the goal can be proved in 0 hops based on a hallucinated fact.
- The existence of the word “not” in the goal is highly predictive of the label.
- Goals having “not” are mostly DISPROVED.
- Goals not having “not” are mostly PROVED.
- The PUD case solves the latter issue to a large extent.
- The label for a good portion of the examples with or without “not” is UNKNOWN.
- The spurious correlations also explain the fluctuations in the CoT performance across different depths.
- The performance depends on how much those correlations appear in the few-shot demonstrations.
- For SI and LAMBADA, such spurious correlations between the input and the label cannot be exploited.
- The intermediate modules are impervious to the correlations between the input and the label.
- Algorithms such as SI that are based on forward chaining require a combinatorial search of the theory to find the right subset of facts and rules in each step of the reasoning.
- The search space becomes progressively larger as the algorithm makes new inferences.
- Those inferences are added back to the theory.
- If the initial size of the theory is |C|, when making the k-th inference the size of the theory is |C| + k − 1.
- The initial size of the theory is the number of facts plus the number of rules.
- As the model produces more inferences, the distance to the goal (in terms of the number of hops remaining between the goal and the facts) should reduce.
- The later inferences should be more accurate.
- The increase in the size of the theory (and hence the size of the search space) may result in lower success rates in the later inferences of the SI model.
- To verify this experimentally, the results of SI on depth-5 of PrOntoQA were further analyzed.
- The subset of examples where the label was PROVED but SI failed to find a proof was extracted.
- These are examples where at least one of the inferences is not on the proof chain.
- As a proxy for measuring the responsibility of the k-th inference of the model for the failure, the percentage of times the k-th inference was on the proof chain was measured.
- The proof chain for each test example is provided as part of the dataset.
- It is possible that the first inference is not on the proof chain, but the rest of the inferences are.
- The results are reported in Figure 3 in the main text.
- The results show that the chance of producing inferences that are on the proof chain progressively decreases in the later inferences of the model.
- The following were proved, disproved, and unknown.
- The predicted label was proved.
- The predicted label was disproved.
- The predicted label was unknown.
- The true label was 84.
- The true label was 82.
- The true label was 331.
- The true label was 23.
- The true label was 169.
- The true label was 14.
- The true label was 180.
- The true label was 53.
- The true label was 64.
- Chain Of Thought (a) ProofWriter-PUD (Depth-5) was used.
- The following were proved, disproved, and unknown.
- The predicted label was proved.
- The predicted label was disproved.
- The predicted label was unknown.
- The true label was 34.
- The true label was 15.
- The true label was 413.
- The true label was 0.
- The true label was 242.
- The true label was 4.
- The true label was 257.
- The true label was 19.
- The true label was 15.
- Selection Inference (b) ProofWriter-PUD (Depth-5) was used.
- The following were proved, disproved, and unknown.
- The predicted label was proved.
- The predicted label was disproved.
- The predicted label was unknown.
- The true label was 163.
- The true label was 149.
- The true label was 410.
- The true label was 2.
- The true label was 111.
- The true label was 0.
- The true label was 127.
- The true label was 14.
- The true label was 24.
- LAMBADA (c) ProofWriter-PUD (Depth-5) was used.
- The following were proved and disproved.
- The predicted label was proved.
- The predicted label was disproved.
- The true label was 169.
- The true label was 110.
- The true label was 36.
- The true label was 67.
- Chain Of Thought (d) PrOntoQA (Depth-5) was used.
- The following were proved and disproved.
- The predicted label was proved.
- The predicted label was disproved.
- The true label was 6.
- The true label was 176.
- The true label was 209.
- The true label was 9.
- Selection Inference (e) PrOntoQA (Depth-5) was used.
- The following were proved and disproved.
- The predicted label was proved.
- The predicted label was disproved.
- The true label was 203.
- The true label was 183.
- The true label was 12.
- The true label was 2.
- LAMBADA (f) PrOntoQA (Depth-5) was used.
- The following were proved, disproved, and unknown.
- The predicted label was proved.
- The predicted label was disproved.
- The predicted label was unknown.
- The true label was 112.
- The true label was 95.
- The true label was 91.
- The true label was 3.
- The true label was 10.
- The true label was 8.
- The true label was 22.
- The true label was 87.
- The true label was 72.
- Chain Of Thought (g) ParaRules was used.
- The following were proved, disproved, and unknown.
- The predicted label was proved.
- The predicted label was disproved.
- The predicted label was unknown.
- The true label was 222.
- The true label was 224.
- The true label was 416.
- The true label was 8.
- The true label was 20.
- The true label was 6.
- The true label was 20.
- The true label was 40.
- The true label was 44.
- LAMBADA (h) ParaRules was used.
- Figure 11: Confusion matrices is the figure.
- The size of the input theory and hence the search space is larger.
- B.4 Confusion Matrices is a section.
- The overall model accuracies were reported in the main text.
- Finer-grained confusion matrices that help better understand the biases of the model were reported.
- Figure 11 reports the confusion matrices for our datasets.
- According to the results, we observe that whenever LAMBADA predicts PROVED or DISPROVED, the prediction is mostly correct.
- The accuracy is slightly more on cases where the prediction is PROVED than DISPROVED.
- This is because DISPROVED cases typically involve negation that makes the reasoning more complex.
- There are several examples for which the label is PROVED or DISPROVED, whereas the model predicts UNKNOWN.
- CoT and SI also show similar behaviour as LAMBADA on ProofWriter-PUD but with a larger bias toward prediction UNKNOWN.
- Moreover, SI shows a large tendency toward predicting DISPROVED for PrOntoQA.
- B.5 Lexical Sensitivity Analysis is a section.
- To analyze the lexical sensitivity of LAMBADA, a new test for ProofWriter-PUD was created.
- The new test contains tokens that do not appear in demonstration examples.
- Specifically, a pool of entity names, animal names, adjectives, and verbs (all of them previously not appearing in the ProofWriter dataset) was manually created.
- Then the following was done.
```
Depth-0 Depth-1 Depth-2 Depth-3 Depth-5 is a label.
0.0 is a value.
0.2 is a value.
0.4 is a value.
0.6 is a value.
0.8 is a value.
1.0Accuracy is a label.
Original T est Set is a label.
Novel T oken T est Set (v1) is a label.
Novel T oken T est Set (v2) is a label.
(a) is a label.
Depth-0 Depth-1 Depth-2 Depth-3 Depth-5 is a label.
0.0 is a value.
0.2 is a value.
0.4 is a value.
0.6 is a value.
0.8 is a value.
1.0Accuracy is a label.
Original T est Set is a label.
Novel T emplate T est Set (v1) is a label.
Novel T emplate T est Set (v2) (b) is a label.
Figure 12 shows the performance of LAMBADA on ProofWriter-PUD.
The performance is for (a) the original and the novel token test sets.
The performance is for (b) the original and the novel template test sets.
The results show that LAMBADA is robust to lexical and template modifications.
The system identified all entity names.
The system mapped each entity name to a randomly selected name from the pool.
The system identified all animals.
The system mapped each of the animals to a randomly selected animal from the pool.
The system identified all adjectives.
The system mapped each of the adjectives to a randomly selected adjective from the pool.
The system identified all verbs.
The system mapped each of the verbs (except the to be verbs) to a randomly selected verb from the pool.
For example, dogmay be mapped to bison in one example.
dogmay be mapped to camel in another example.
Then, the system tested the performance of LAMBADA on this modified test set.
The system compared the results to the original test set.
The system also analyzed the sensitivity to the templates used for the rules.
Toward this goal, the system identified the templates used for the rules in the ProofWriter dataset.
The system replaced each template with another template (previously not appearing in the ProofWriter dataset).
For example, the system changed the template “[X] things are [Y]” to “It is a truth that [X] things are always [Y] as well”.
Then, the system tested the performance of LAMBADA on this modified test set.
The system compared the results to the original test set.
The system repeated the aforementioned experiments twice for each analysis.
The system used a different set of tokens/templates each time.
The results in Figure 8 in the main text demonstrate the average accuracy across two runs.
The results for individual runs are presented in Figure 12(a), (b) for the two analyses respectively.
According to the results, the system observes some variations in the total accuracy.
For some depths the performance goes slightly down.
For some depths the performance goes slightly up.
The performance stays in the same ballpark, showing the robustness of LAMBADA.
Moreover, the system compared the results on the modified test set with those of the baselines reported in the main text.
The system observes that even on this modified test set, LAMBADA performs significantly better than the baselines tested on the original test set.
C is a label.
Combinatorial Search Issue in Forward Chaining is a label.
Consider a simple fictional theory with the following facts: [Anne is cold., Anne is nice and pink., Anne is kind., Anne is green., Anne is big and young., Anne is rough., Anne is round.]
the following rules: [Cold, red people are white. , Nice, blue people are white., Kind, green people are white., Cold, round people are white., Big, green people are white.]
the goal is “Anne is white.”
An approach based on forward chaining requires selecting a subset of the facts and rules from the theory from which this goal can be proved.
Specifically, the approach needs to select “Anne is cold.”, “Anne is round.”, and Cold, round people are white. from the theory.
Such a selection requires a combinatorial search.
Different combinations of facts and rules should be tested to see which one can lead to proving the goal.
An LM may fail to search this space effectively in a single inference call.
SI uses an approximation to reduce the search space.
SI first makes an inference call to an LM to select one fact/rule.
SI makes another inference call to select the next fact/rule based on the first one.
SI continues to make inference calls until a halting criterion is met.
```
- The search space is reduced from a combinatorial space to a linear space.
- The facts/rules are not selected jointly.
- The chances of selecting the wrong combinations of facts and rules increase because repairing a wrong first choice is not possible.
- This leads to low performance.
- Low performance is evidenced in experimental results.
- With a backward chaining approach such as LAMBADA, no combinatorial search (or approximations to it) is required.
- The Rule Selection module verifies each rule independently to see which one is applicable (i.e. a linear scan).
- The Goal Decomposition module breaks goals into sub-goals based on each selected rule independently of the other selected rules.
- The Fact Check module verifies the existence of a fact that entails or contradicts the goal with a linear search over the facts.
- For experiments, the PaLM 540B model (Chowdhery et al., 2022) was used for all the models (both LAMBADA and the baselines).
- The PaLM 540B model was served on a 4 × 4 TPU v4 architecture.
- The decoding temperature was set to zero.
- For testing CoT on PrOntoQA, the same demonstration examples as the original work were used.
- The wording was slightly changed by adding conjunctive words such as “Since” and “So” to make the chains have a better flow.
- The reason for this modification was that it was found that when working with PaLM, prompts that have a better flow result in better predictions.
- This can be viewed from Figure 13 where the performance for the original prompts vs. the prompts with the conjunctive words added is compared.
- The latter slightly underperforms on Depth-1 (where the reasoning flow is not as important).
- The latter substantially improves the results for higher depths (especially Depth-5).
- For ProofWriter, similar few-shot examples were written.
- For SI, the same demonstration examples as in the original work for ProofWriter were used.
- For PrOntoQA, few-shot examples following a similar pattern to those for ProofWriter were written.
- For each dataset depth, specific few-shot examples were used/written (e.g., when working with a subset of the data that has examples requiring at most k hops of reasoning, the CoT demonstrations also require only k hops of reasoning).
- For ProofWriter Depth-5, following the original work, it was used for testing length-generalization and only included examples with chains up to 3 hops.
- For running CoT on ProofWriter-PUD, extra few-shot examples where the label is UNKNOWN were included.
- The explanation for these examples is that the goal cannot be proved or disproved with a combination of the facts and the rules.
- For running SI on ProofWriter-PUD, after obtaining the inferences by running SI, the inferences and the goal were given to the Fact Check module which decides if the goal can be proved, disproved, or neither.
- ProofWriter-PD and PrOntoQA are binary datasets.
- LAMBADA makes three-way predictions (PROVED, DISPROVED, and UNKNOWN).
- To test LAMBADA on these datasets, similar to SI, the UNKNOWN and DISPROVED predictions were combined into one class.
- For creating datasets for measuring the performance of individual modules in LAMBADA, the following was done.
- For Fact Check, 100 examples were randomly selected from the Depth-0 examples.
- A model prediction is counted to be correct if it produces the same label as the one specified in the ProofWriter dataset.
- For Rule Selection, 100 examples were randomly selected and every rule whose consequent unifies with the goal was manually enumerated.
- A model prediction is considered correct if it predicts all such rules correctly.
- For Goal Decomposition, 100 rules and goals were randomly selected such that the consequent of the rule unifies with the goal and then the sub-goals were manually written.
- A model prediction is considered correct if it predicts all the sub-goals correctly.
- For Sign Agreement, the same examples from the Goal Decomposition module were re-used and manually labeled them with respect to their sign agreement/disagreement.
- Chowdhery et al., 2022 is the author of the PaLM 540B model.
- The PaLM 540B model is a model.
Output: The ParaRules dataset has a high amount of variation in the text., The ParaRules dataset has a high amount of variation in the facts., The ParaRules dataset has a high amount of variation in the rules., The ParaRules dataset makes it a valuable benchmark for evaluating text-based logical reasoning., We also found a few quality issues in the ParaRules dataset., The quality issues in the ParaRules dataset were introduced when annotators converted facts and rules into natural language form., Here, we describe some of the main issues that we found and fixed., In some cases the rule was “X and Y imply Z”., The natural language version of the rule produced by annotators was written as if “X implies Y and Z”., The natural language version of the rule produced by annotators was written as if “X implies Y or Z”., As an example, the rule “Cold, nice people are red.” was written in natural language form as “Some cold people can be nice at times,and red at at other times.” ., For such cases, we modified the text to make the antecedents and consequent match the original rule., In some cases, the annotator introduced new antecedents in the rule., For example, for a rule where the antecedents were “green”, “red” and “rough”, the annotator added another antecedent “naive”., If someone is green and naive ..., For such cases, we removed the extra antecedents., In some cases, the natural language version of a general rule was written for only a specific entity., For example the rule “Rough, young, green people are very round.” was written as “Tom is a rough, young person to know ...” ., We removed the specific entities and made the rule generally applicable., For some of the facts, the annotator replaced the name of the entity with a pronoun., As an example, “Dave is ...” was annotated as “He is ...”., We replaced the pronouns with the original entity name in the theory., We provide an overview of the prompts we used for each of the four components of our model for the ProofWriter dataset., The pseudo-code for the Fact Check module is provided in Algorithm 3., For selecting a fact in Fact Check, our prompt looks like the following:, Algorithm 3 FactCheck, Input: Facts F, Goal G, Number of trials n, for n times do do, f = FactSelection(F, G), result = FactVerifier(f,G), if result ̸= UNKNOWN then, return result, F = F −f, return UNKNOWN, Algorithm 4 RuleSelection, Input: Rules R, Goal G, I = RuleImplications(R), selected = SelectRules(I, G), return selected, Example 1, Fact1: <FACT1> Fact2: <FACT2> ..., Factn: <FACTn>, Question: <QUESTION>, Inference: For the question <QUESTION> the most relevant fact is Facti (<FACTi>)., Example K, Fact1: <FACT> Fact2: <FACT> ... Factm: <FACT>, Question: <QUESTION>, Inference:, For verifying if the goal/question can be derived from the selected fact, we use the following prompt:, Example 1, Fact: <FACT>, Question: <QUESTION>, Inference: The fact <FACT> [X1] the question <QUESTION> so [X2]., Example K, Fact: <FACT>, Question: <QUESTION>, Inference:, In the case where the goal can be proved from the fact, we replace [X1] with “is equivalent to” and [X2] with “so the answer is yes” ., In the case where the goal can be disproved from the fact, we replace [X1]with “is the negation of” and [X2] with “so the answer is no”., And in the case where the goal can neither be proved
The question cannot be inferred from the fact.
We replace [X1] with 'is neither equivalent nor the negation of'.
We replace [X2] with 'so the question cannot be inferred from the fact'.
The pseudo-code for the Rule Selection module is provided in Algorithm 4.
For finding the implication/consequent of the rules, we use the following prompt:
Example 1
Rule1: <RULE1>, Rule2: <RULE2> ...
Rulen: <RULEn>
Inference: Rule1 implies [X1], . . ., Rulen implies [Xn].
Example K
Rule1: <RULE1>, Rule2: <RULE2> ...
Rulem: <RULEm>
Inference: [Xi]s depend on the consequent of each rule.
For rules such as 'Rough, nice people are red.' we write [Xi] as '(is; red)'.
For rules such as 'If the cat chases the dog then the cat sees the dog.' we write [Xi] as '(cat; chase; dog)'.
For rule selection based on the implications, we use the following prompt:
Example 1
Rule1 implies <IMLP1>, Rule2 implies <IMPL2>, ..., Rulen implies <IMPLn>
Question: <QUESTION>
Inference: The question is about <IMPLq>: Rule1 <IMPL1> [X1] <IMPLq>, . . ., <IMPLn> [Xn] <IMPLq>.
Example K
Rule1 implies <IMLP1>, Rule2 implies <IMPL2>, ..., Rulem implies <IMPLm>
Question: <QUESTION>
Inference: where each [X1] is either 'is applicable to' or 'not applicable to' depending on whether the rule can be applied or not.
For goal decomposition, we use the following prompt:
Example 1
Rule: <Rule>
Question: <QUESTION>
Inference: The question subject is <SUBJq> and the rule premises are <PRM>*, so the question breaks down to <SUBQ>*.
Example K
Rule: <RULE>
Question: <QUESTION>
Inference: where <SUBJq> indicates the subject of the question, <PRM>* indicates the premises/antecedents in the rule (the * indicates that there might be multiple premises), and <SUBQ>* indicates the sub-goals.
Finally, for sign agreement, we use the following prompt:
Example 1
Rule: <Rule>
Question: <QUESTION>
Inference: The rule implication <IMLPr> is [Xr], the question <IMPLq> is [Xq], so signs [Xd].
Example K
Rule: <RULE>
Question: <QUESTION>
Inference: where <IMLPr> shows the implication of the rule and <IMPLq> indicates the implication of the question.
[Xr] and [Xq] are either 'positive' or 'negated' depending on the sign of the implication.
[Xd] is either 'agree' or 'disagree' depending on whether the signs agree or not.
