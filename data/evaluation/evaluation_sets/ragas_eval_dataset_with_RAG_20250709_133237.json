[
  {
    "question": "What type of problems is the current version of LAMBADA primarily designed to solve?",
    "answer": "Based on the provided context, the current version of LAMBADA is primarily designed to solve **deductive logical reasoning problems with natural language input.**\n\nHere's the reasoning:\n\n*   **Document 4:** States that LAMBADA achieves accuracy boosts on challenging logical reasoning datasets.\n*   **Document 5:** Describes LAMBADA as an algorithm for deductive logical reasoning.\n*   **Document 3:** Highlights future work to extend LAMBADA to non-classification cases, implying the current work focuses on classification, likely related to logical reasoning.\n*   **Document 1:** Talks about problem decomposition, suggesting it's designed to solve complex problems.",
    "contexts": [
      "Document 1:\n. If smaller LMs are\nused, then one may need finer-grained problem de-\ncomposition (e.g., further decomposing the one-to-\nmany comparisons in the selection module). And\nas LMs become larger and stronger in the future,\none could rely on them to solve problems with a\ncoarser-grained decomposition of the problem.\n5.8 Number of Inference Calls\nAnother advantage of LAMBADA is its efficiency\ncompared to other approaches that require multiple\nLM inference calls per example such as SI\n\nDocument 2:\nsearch trace of LAMBADA on an exam-\nple from the ParaRules subset of ProofWriter (the Sign\nAgreement and failed Fact Check modules are omitted\nfor brevity).\nWhile many problems benefit from LM scaling,\nscaling has been observed to provide limited benefit\nfor solving complex reasoning problems\n\nDocument 3:\n. Future work can\nextend LAMBADA to non-classification cases,\ne.g., where one needs to apply logical reasoning\nto answer questions such as “What color is\nFiona?”.\n• The current work assumes all the rules are given\nas input and the rule set is small enough to be\nincluded in the prompt\n\nDocument 4:\n. We show that LAMBADA achieves siz-\nable accuracy boosts over state-of-the-art for-\nward reasoning methods on challenging logical\nreasoning datasets, particularly when deep and\naccurate proof chains are required.\n1 Introduction\nAutomated reasoning, the ability to draw valid con-\nclusions from explicitly provided knowledge, has\nbeen a fundamental goal for AI since its early\ndays (McCarthy, 1959; Hewitt, 1969)\n\nDocument 5:\n. As\ncan be seen in Figure 8, the performance of LAM-\nBADA remains almost unchanged, demonstrating\nrobustness to lexical and templatic variations.\n6 Conclusion and Future Directions\nWe developed LAMBADA , an algorithm for deduc-\ntive logical reasoning with natural language that\ncombines the capacity of LMs to handle natural-\nistic text input with the backward chaining algo-\nrithm for robust symbolic reasoning\n"
    ],
    "ground_truth": "The current version of LAMBADA is mainly applicable to logical entailment problems that involve classification."
  },
  {
    "question": "According to the document, what is the accuracy of LAMBADA's predictions when it predicts PROVED or DISPROVED?",
    "answer": "Based on Document 1, when LAMBADA predicts PROVED or DISPROVED, the prediction is mostly correct. The accuracy is slightly higher for PROVED predictions than for DISPROVED predictions.",
    "contexts": [
      "Document 1:\n. Here, we report finer-grained confusion\nmatrices that help better understand the biases of\nthe model. Figure 11 reports the confusion matrices\nfor our datasets. According to the results, we ob-\nserve that whenever LAMBADA predicts PROVED\nor DISPROVED , the prediction is mostly correct.\nThe accuracy is slightly more on cases where the\nprediction is PROVED than DISPROVED . We be-\nlieve this is because DISPROVED cases typically\ninvolve negation that makes the reasoning more\ncomplex\n\nDocument 2:\n. (d5)\nFigure 2: Prediction accuracy results on (a) ProofWriter-PUD (b) ProofWriter-PD, (c) PrOntoQA, and (d) ParaRules\ndatasets. (e) The proof accuracy of CoT and LAMBADA on ProofWriter (Depth-5) for a set of randomly sampled\nexamples for which the models correctly predicted if the goal can be proved or disproved.\nwith examples labeled UNKNOWN removed (for\ncompatibility with previous work), and (2) with all\nthree labels\n\nDocument 3:\n. We showed\nthat LAMBADA achieves significant improvements\nover competitive approaches on challenging bench-\nmarks, both in terms of label accuracy (predicting\nif a statement can be proved or disproved based\non a theory) and proof accuracy. Importantly, this\nimprovement was also observed in a dataset that ex-\npresses the theory in more naturalistic expressions,\nclearly illustrating the benefit of combining an LM\nwith reasoning modules\n\nDocument 4:\n. If it does,\nthen the algorithm returns PROVED and otherwise\nDISPROVED . If there is no rule for which the sub-\ngoals can be proved, then UNKNOWN is returned.\nDuring a proof, LAMBADA may be called mul-\ntiple times with the same theory and goal; in Ap-\npendix A we explain how cycles and redundant\ncomputations can be avoided using a cache.\n4 Experimental Setup\nWe describe our baselines and datasets here, and\nprovide further implementation details in Ap-\npendix D\n\nDocument 5:\n. For running\nSI on ProofWriter-PUD, after obtaining the infer-\nences by running SI, we give the inferences and\nthe goal to our Fact Check module which decides\nif the goal can be proved, disproved, or neither.\nSince ProofWriter-PD and PrOntoQA are binary\ndatasets but LAMBADA makes three-way predic-\ntions (PROVED , DISPROVED , and UNKNOWN ), to\ntest LAMBADA on these datasets, similar to SI we\ncombine the UNKNOWN and DISPROVED predic-\ntions into one class.\nD.1 Datasets for Individual\n"
    ],
    "ground_truth": "When LAMBADA predicts PROVED or DISPROVED, the prediction is mostly correct, with slightly higher accuracy for PROVED predictions."
  },
  {
    "question": "According to the document, what happens to the success rate of SI as the inferences progress?",
    "answer": "Based on the provided documents, the success rate of the SI model decreases as the inferences progress. This is explicitly stated in multiple documents, particularly:\n\n*   **Document 1 & 2:** Figure 3 shows that as 'k' (representing the inference number) increases, the success rate of SI decreases. This is attributed to the increasing size of the input theory and the resulting larger search space.\n*   **Document 3:** This document also alludes to the fact that later inferences of SI have lower success rates.",
    "contexts": [
      "Document 1:\n1st 2nd 3rd 4th 5th\nk-th inference\n0.30\n0.35\n0.40\n0.45\n0.50Success Rate\n0.53\n0.47\n0.34\n0.31 0.31\nFigure 3: The success rate of the k-th inference of SI\non PrOntoQA (Depth-5) for different values of k. As\nk increases, the size of the input theory becomes larger\nand the success rate decreases.\nthe desired outcome of combining the strengths of\nan LM and a symbolic reasoning algorithm.\nThe results in Figure 2(a) reveal a shortcoming\nof the CoT approach in dealing with UNKNOWN\nlabels\n\nDocument 2:\n. To verify whether the increase in the search\nspace makes forward chaining progressively harder,\nwe measured the success rate of the k-th inference\nof SI for different values of k on Depth-5 of PrOn-\ntoQA (see Appendix B.3 for details). From the\nresults in Figure 3, we can see that the success\nrate indeed decreases in the later inferences of the\nmodel, where the size of the input theory is larger\nand therefore a larger space needs to be searched to\nfind the right combination of facts and rules\n\nDocument 3:\n. However, we hypothe-\nsize that the increase in the size of the theory (and\nhence the size of the search space) may result in\nlower success rates in the later inferences of the SI\nmodel. To verify this experimentally, we further\nanalyzed the results of SI on depth-5 of PrOntoQA\nas follows. We extracted the subset of examples\nwhere the label was PROVED but SI failed to find\na proof (these are examples where at least one of\nthe inferences is not on the proof chain)\n\nDocument 4:\n. An LM may fail to search this\nspace effectively in a single inference call.\nSI uses an approximation to reduce the search\nspace: it first makes an inference call to an LM to\nselect one fact/rule, then it makes another inference\ncall to select the next fact/rule based on the first\none, and continues to make inference calls until\na halting criterion is met. This approximation re-\n\nDocument 5:\nin LAMBADA ) is a better choice compared\nto forward chaining (the backbone in SI).\n7%\n10%\n20%\n34%\n29%\n1 unique inference\n2 unique inferences\n3 unique inferences\n4 unique inferences\n5 unique inferences\nFigure 4: Number of unique inferences generated by\nSI for Depth-5 of ProofWriter-PUD when selection and\ninference modules are called five times.\n"
    ],
    "ground_truth": "The success rate of SI decreases in the later inferences of the model."
  },
  {
    "question": "What are the three main reasons that CoT produces incorrect proof chains, even when the predicted label is correct?",
    "answer": "Based on Document 1, the three dominant reasons for CoT producing incorrect proof chains, even when the predicted label is correct, are:\n\n1.  **Hallucinating rules or facts:** CoT invents or fabricates rules or factual information that doesn't exist.\n2.  **Not understanding conjunction:** CoT struggles with the logical connective \"and,\" potentially leading to incorrect inferences.\n3.  **Making invalid derivations:** CoT performs logical steps that are not valid, leading to errors in the proof chain.",
    "contexts": [
      "Document 1:\n. Through manually analyzing 50\nexamples for which CoT predicted the correct la-\nbel, we identified three dominant reasons for the\nchains being wrong: 1- hallucinating rules or facts,\n2- not understanding conjunction, and 3- making\ninvalid derivations. In Figure 10, we show failure\nexamples from each category\n\nDocument 2:\n. The results are reported in Figure 2(e).\nWhile LAMBADA mostly produces correct\nchains, CoT produces correct chains only for 28%\nof the examples. We find that hallucination is the\nmain source of error (48% of the examples; see Ap-\npendix B.2 for other prominent failure modes). The\nhallucinated facts and rules mostly resulted in short-\ncuts to the correct answer\n\nDocument 3:\nsign of the rules and\nthe sub-goals agree for all cases, except for the very\nfirst rule selected (Rule3) so it correctly concludes\nthat the goal is disproved.\nB.2 Further Analysis of CoT\nIn Figure 2(e), we observed that CoT mostly pro-\nduces wrong proof chains even when the predicted\nlabel is correct\n\nDocument 4:\n. We investigate the reason for this\nbehaviour of CoT in the next section.\n5.2 Proof Accuracy\nTo understand the reason behind the high accuracy\nof CoT on higher depths of ProofWriter-PD, we\nrandomly selected 50 examples from Depth-5 of\nthe dataset where CoT predicted the label correctly,\nand manually verified if the proof chain is correct or\nnot. For comparison, we also manually verified the\nproofs generated by LAMBADA following a similar\nprocedure\n\nDocument 5:\n. That is, unlike the examples for which the\nlabel is PROVED or DISPROVED , there is no natural\nchain of thought for the examples whose labels are\nUNKNOWN . Nevertheless, the performance of CoT\nis competitive for the ProofWriter-PD dataset, and\nthe accuracy does not diminish substantially with\nincreasing depth\n"
    ],
    "ground_truth": "The three dominant reasons for CoT producing wrong proof chains are hallucinating rules or facts, not understanding conjunction, and making invalid derivations."
  },
  {
    "question": "What type of algorithm is LAMBADA?",
    "answer": "Based on the provided documents, LAMBADA is an algorithm for deductive logical reasoning with natural language. It combines the capabilities of Language Models (LMs) to handle naturalistic text input with the backward chaining algorithm for robust symbolic reasoning.",
    "contexts": [
      "Document 1:\n. As\ncan be seen in Figure 8, the performance of LAM-\nBADA remains almost unchanged, demonstrating\nrobustness to lexical and templatic variations.\n6 Conclusion and Future Directions\nWe developed LAMBADA , an algorithm for deduc-\ntive logical reasoning with natural language that\ncombines the capacity of LMs to handle natural-\nistic text input with the backward chaining algo-\nrithm for robust symbolic reasoning\n\nDocument 2:\n. This motivates the fourth\nmodule, Sign Agreement, described below.\nGiven a rule r and a goal G, the Sign Agreement\nmodule verifies if the sign of the consequent of r\nagrees or disagrees with the sign of the goal or not.\n3.3 The L AMBADA Algorithm\nAlgorithm 1 provides a high-level description of\nhow the four LM modules described earlier can\nbe integrated with BC to enable text-based logical\nreasoning (the function calls corresponding to LM\nmodules are color-coded).\nLAMBADA can be understood as\n\nDocument 3:\n. Otherwise,\nthe algorithm proceeds with applying rules.\nThe Rule Selection module is used to identify the\nrules Rs from R whose consequent unifies with G.\nOnce the setRs is identified, if LAMBADA can start\nwith the rules that have a higher chance of succeed-\ning at (dis)proving the goal, it can save computa-\ntions and be less error-prone. Therefore, we include\na Rerank function in LAMBADA\n\nDocument 4:\n. We\nshow one such example in Figure 9 (to be discussed\nin Section B).\nLAMBADA may sometimes run into loops.\nFor example, to prove a (sub-)goal “Fiona is\nround?”, after recursively identifying rules that\nunify with it and decomposing it into sub-goals,\nthe algorithm may arrive at a point where it needs\nto prove the “Fiona is round?” sub-goal, which\nis equivalent to the initial goal\n\nDocument 5:\n. If it does,\nthen the algorithm returns PROVED and otherwise\nDISPROVED . If there is no rule for which the sub-\ngoals can be proved, then UNKNOWN is returned.\nDuring a proof, LAMBADA may be called mul-\ntiple times with the same theory and goal; in Ap-\npendix A we explain how cycles and redundant\ncomputations can be avoided using a cache.\n4 Experimental Setup\nWe describe our baselines and datasets here, and\nprovide further implementation details in Ap-\npendix D\n"
    ],
    "ground_truth": "LAMBADA is a recursive algorithm."
  }
]