[
  {
    "question": "What type of problems is the current version of LAMBADA primarily designed to solve?",
    "answer": "Based on the provided documents, the current version of LAMBADA is primarily designed to solve **logical reasoning problems** that require **drawing valid conclusions from explicitly provided knowledge**.\n\nSeveral clues point to this:\n\n*   **Document 3:** States that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.\n*   **Document 6:** Describes LAMBADA as a way to integrate LM modules with Backward Chaining (BC) to enable text-based logical reasoning.\n*   **Document 4:** Mentions an example of LAMBADA trying to prove a goal, indicating its use in logical deduction.\n*   **Document 8:** Suggests future work to extend LAMBADA to non-classification cases, implying that the current version is focused on classification problems within the realm of logical reasoning.",
    "contexts": [
      "Document 1:\n. If smaller LMs are\nused, then one may need finer-grained problem de-\ncomposition (e.g., further decomposing the one-to-\nmany comparisons in the selection module). And\nas LMs become larger and stronger in the future,\none could rely on them to solve problems with a\ncoarser-grained decomposition of the problem.\n5.8 Number of Inference Calls\nAnother advantage of LAMBADA is its efficiency\ncompared to other approaches that require multiple\nLM inference calls per example such as SI\n\nDocument 2:\n. The Sign Agreement module also\nshows near-perfect accuracy.\n5.7 The Role of Scale\nWe repeat the experiment from Section 5.6 with\nPaLM 62B and 8B to examine the effect of LM\nscale on LAMBADA . According to the results in\nFigure 6, when we use PaLM 62B, the performance\nof the Goal Decomposition and Sign Agreement\nmodules remain comparable, but the performance\nfor the Fact Check and Rule Selection modules\ndrop substantially\n\nDocument 3:\n. We show that LAMBADA achieves siz-\nable accuracy boosts over state-of-the-art for-\nward reasoning methods on challenging logical\nreasoning datasets, particularly when deep and\naccurate proof chains are required.\n1 Introduction\nAutomated reasoning, the ability to draw valid con-\nclusions from explicitly provided knowledge, has\nbeen a fundamental goal for AI since its early\ndays (McCarthy, 1959; Hewitt, 1969)\n\nDocument 4:\n. We\nshow one such example in Figure 9 (to be discussed\nin Section B).\nLAMBADA may sometimes run into loops.\nFor example, to prove a (sub-)goal “Fiona is\nround?”, after recursively identifying rules that\nunify with it and decomposing it into sub-goals,\nthe algorithm may arrive at a point where it needs\nto prove the “Fiona is round?” sub-goal, which\nis equivalent to the initial goal\n\nDocument 5:\n. The results show that LAMBADA is robust to lexical and template\nmodifications.\ning modifications for each example: 1- identified\nall entity names and mapped each entity name to\na randomly selected name from the pool, 2- iden-\ntified all animals and mapped each of them to a\nrandomly selected animal from the pool, 3- iden-\ntified all adjectives and mapped each of them to\na randomly selected adjective from the pool, and\n4- identified all verbs and mapped each of them\n(except the to be verbs) to a\n\nDocument 6:\n. This motivates the fourth\nmodule, Sign Agreement, described below.\nGiven a rule r and a goal G, the Sign Agreement\nmodule verifies if the sign of the consequent of r\nagrees or disagrees with the sign of the goal or not.\n3.3 The L AMBADA Algorithm\nAlgorithm 1 provides a high-level description of\nhow the four LM modules described earlier can\nbe integrated with BC to enable text-based logical\nreasoning (the function calls corresponding to LM\nmodules are color-coded).\nLAMBADA can be understood as\n\nDocument 7:\n. LAMBADA requires much fewer calls com-\npared to SI, especially at higher depths: for Depth-\n1, LAMBADA requires 3.8x fewer calls whereas for\nDepth-5 it requires 11.8x fewer calls.\n5.9 Lexical Robustness\nTo analyze the lexical sensitivity ofLAMBADA , we\nmodified the test set of ProofWriter-PUD by replac-\ning various lexical items (names, adjectives, and\nverbs) with novel tokens and the rule templates with\nnovel ones\n\nDocument 8:\n. Future work can\nextend LAMBADA to non-classification cases,\ne.g., where one needs to apply logical reasoning\nto answer questions such as “What color is\nFiona?”.\n• The current work assumes all the rules are given\nas input and the rule set is small enough to be\nincluded in the prompt\n\nDocument 9:\n. Here, we report finer-grained confusion\nmatrices that help better understand the biases of\nthe model. Figure 11 reports the confusion matrices\nfor our datasets. According to the results, we ob-\nserve that whenever LAMBADA predicts PROVED\nor DISPROVED , the prediction is mostly correct.\nThe accuracy is slightly more on cases where the\nprediction is PROVED than DISPROVED . We be-\nlieve this is because DISPROVED cases typically\ninvolve negation that makes the reasoning more\ncomplex\n\nDocument 10:\nof modules in\nLAMBADA in isolation, for different LM sizes.\n5.4 Does Backward CoT Suffice?\nOur results may raise the question of whether it is\nenough to directly incorporate the steps of back-\nward chaining into CoT prompts, or if modularity\n(as in LAMBADA ) is also needed\n"
    ],
    "ground_truth": "The current version of LAMBADA is mainly applicable to logical entailment problems that involve classification."
  }
]