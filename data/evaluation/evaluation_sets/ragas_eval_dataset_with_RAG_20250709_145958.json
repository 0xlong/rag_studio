[
  {
    "question": "What type of problems is the current version of LAMBADA primarily designed to solve?",
    "answer": "Based on the provided documents, the current version of LAMBADA is primarily designed to solve **logical reasoning problems** that require drawing valid conclusions from explicitly provided knowledge. This is explicitly stated in Document 3: \"Automated reasoning, the ability to draw valid conclusions from explicitly provided knowledge...\" and the context of the other documents suggests this is the core functionality of LAMBADA.\n\nSpecifically, it excels at problems that:\n\n*   Require deep and accurate proof chains (Document 3).\n*   Can be framed as classification problems (Document 8 and Document 9, which discusses PROVED/DISPROVED predictions).\n*   Involve text-based logical reasoning (Document 6).\n\nThe documents also highlight that LAMBADA's performance is affected by the size of the language model used and that it can sometimes run into loops (Document 4), so it is not a perfect solution for all logical reasoning problems.",
    "contexts": [
      "Document 1:\n. If smaller LMs are\nused, then one may need finer-grained problem de-\ncomposition (e.g., further decomposing the one-to-\nmany comparisons in the selection module). And\nas LMs become larger and stronger in the future,\none could rely on them to solve problems with a\ncoarser-grained decomposition of the problem.\n5.8 Number of Inference Calls\nAnother advantage of LAMBADA is its efficiency\ncompared to other approaches that require multiple\nLM inference calls per example such as SI\n\nDocument 2:\n. The Sign Agreement module also\nshows near-perfect accuracy.\n5.7 The Role of Scale\nWe repeat the experiment from Section 5.6 with\nPaLM 62B and 8B to examine the effect of LM\nscale on LAMBADA . According to the results in\nFigure 6, when we use PaLM 62B, the performance\nof the Goal Decomposition and Sign Agreement\nmodules remain comparable, but the performance\nfor the Fact Check and Rule Selection modules\ndrop substantially\n\nDocument 3:\n. We show that LAMBADA achieves siz-\nable accuracy boosts over state-of-the-art for-\nward reasoning methods on challenging logical\nreasoning datasets, particularly when deep and\naccurate proof chains are required.\n1 Introduction\nAutomated reasoning, the ability to draw valid con-\nclusions from explicitly provided knowledge, has\nbeen a fundamental goal for AI since its early\ndays (McCarthy, 1959; Hewitt, 1969)\n\nDocument 4:\n. We\nshow one such example in Figure 9 (to be discussed\nin Section B).\nLAMBADA may sometimes run into loops.\nFor example, to prove a (sub-)goal “Fiona is\nround?”, after recursively identifying rules that\nunify with it and decomposing it into sub-goals,\nthe algorithm may arrive at a point where it needs\nto prove the “Fiona is round?” sub-goal, which\nis equivalent to the initial goal\n\nDocument 5:\n. The results show that LAMBADA is robust to lexical and template\nmodifications.\ning modifications for each example: 1- identified\nall entity names and mapped each entity name to\na randomly selected name from the pool, 2- iden-\ntified all animals and mapped each of them to a\nrandomly selected animal from the pool, 3- iden-\ntified all adjectives and mapped each of them to\na randomly selected adjective from the pool, and\n4- identified all verbs and mapped each of them\n(except the to be verbs) to a\n\nDocument 6:\n. This motivates the fourth\nmodule, Sign Agreement, described below.\nGiven a rule r and a goal G, the Sign Agreement\nmodule verifies if the sign of the consequent of r\nagrees or disagrees with the sign of the goal or not.\n3.3 The L AMBADA Algorithm\nAlgorithm 1 provides a high-level description of\nhow the four LM modules described earlier can\nbe integrated with BC to enable text-based logical\nreasoning (the function calls corresponding to LM\nmodules are color-coded).\nLAMBADA can be understood as\n\nDocument 7:\n. LAMBADA requires much fewer calls com-\npared to SI, especially at higher depths: for Depth-\n1, LAMBADA requires 3.8x fewer calls whereas for\nDepth-5 it requires 11.8x fewer calls.\n5.9 Lexical Robustness\nTo analyze the lexical sensitivity ofLAMBADA , we\nmodified the test set of ProofWriter-PUD by replac-\ning various lexical items (names, adjectives, and\nverbs) with novel tokens and the rule templates with\nnovel ones\n\nDocument 8:\n. Future work can\nextend LAMBADA to non-classification cases,\ne.g., where one needs to apply logical reasoning\nto answer questions such as “What color is\nFiona?”.\n• The current work assumes all the rules are given\nas input and the rule set is small enough to be\nincluded in the prompt\n\nDocument 9:\n. Here, we report finer-grained confusion\nmatrices that help better understand the biases of\nthe model. Figure 11 reports the confusion matrices\nfor our datasets. According to the results, we ob-\nserve that whenever LAMBADA predicts PROVED\nor DISPROVED , the prediction is mostly correct.\nThe accuracy is slightly more on cases where the\nprediction is PROVED than DISPROVED . We be-\nlieve this is because DISPROVED cases typically\ninvolve negation that makes the reasoning more\ncomplex\n\nDocument 10:\nof modules in\nLAMBADA in isolation, for different LM sizes.\n5.4 Does Backward CoT Suffice?\nOur results may raise the question of whether it is\nenough to directly incorporate the steps of back-\nward chaining into CoT prompts, or if modularity\n(as in LAMBADA ) is also needed\n"
    ],
    "ground_truth": "The current version of LAMBADA is mainly applicable to logical entailment problems that involve classification."
  },
  {
    "question": "According to the document, what is the accuracy of LAMBADA's predictions when it predicts PROVED or DISPROVED?",
    "answer": "Document 1 states that \"whenever LAMBADA predicts PROVED or DISPROVED, the prediction is mostly correct. The accuracy is slightly more on cases where the prediction is PROVED than DISPROVED.\"\n\nTherefore, according to the document, LAMBADA's predictions are mostly correct when it predicts PROVED or DISPROVED.",
    "contexts": [
      "Document 1:\n. Here, we report finer-grained confusion\nmatrices that help better understand the biases of\nthe model. Figure 11 reports the confusion matrices\nfor our datasets. According to the results, we ob-\nserve that whenever LAMBADA predicts PROVED\nor DISPROVED , the prediction is mostly correct.\nThe accuracy is slightly more on cases where the\nprediction is PROVED than DISPROVED . We be-\nlieve this is because DISPROVED cases typically\ninvolve negation that makes the reasoning more\ncomplex\n\nDocument 2:\n. (d5)\nFigure 2: Prediction accuracy results on (a) ProofWriter-PUD (b) ProofWriter-PD, (c) PrOntoQA, and (d) ParaRules\ndatasets. (e) The proof accuracy of CoT and LAMBADA on ProofWriter (Depth-5) for a set of randomly sampled\nexamples for which the models correctly predicted if the goal can be proved or disproved.\nwith examples labeled UNKNOWN removed (for\ncompatibility with previous work), and (2) with all\nthree labels\n\nDocument 3:\n. Moreover, SI shows\na large tendency toward predicting DISPROVED for\nPrOntoQA.\nB.5 Lexical Sensitivity Analysis\nTo analyze the lexical sensitivity of LAMBADA ,\nwe created a new test for ProofWriter-PUD which\ncontains tokens that do not appear in demonstra-\ntion examples. Specifically, we manually created\na pool of entity names, animal names, adjectives,\nand verbs (all of them previously not appearing in\nthe ProofWriter dataset) and then made the follow-\n\nDocument 4:\n. For running\nSI on ProofWriter-PUD, after obtaining the infer-\nences by running SI, we give the inferences and\nthe goal to our Fact Check module which decides\nif the goal can be proved, disproved, or neither.\nSince ProofWriter-PD and PrOntoQA are binary\ndatasets but LAMBADA makes three-way predic-\ntions (PROVED , DISPROVED , and UNKNOWN ), to\ntest LAMBADA on these datasets, similar to SI we\ncombine the UNKNOWN and DISPROVED predic-\ntions into one class.\nD.1 Datasets for Individual\n\nDocument 5:\nDepth-0 Depth-1 Depth-2 Depth-3 Depth-5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nOriginal T est Set\nNovel T oken T est Set (v1)\nNovel T oken T est Set (v2)\n(a)\nDepth-0 Depth-1 Depth-2 Depth-3 Depth-5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nOriginal T est Set\nNovel T emplate T est Set (v1)\nNovel T emplate T est Set (v2) (b)\nFigure 12: The performance of LAMBADA on ProofWriter-PUD for (a) the original and the novel token test sets,\n(b) the original and the novel template test sets\n\nDocument 6:\n. The Sign Agreement module also\nshows near-perfect accuracy.\n5.7 The Role of Scale\nWe repeat the experiment from Section 5.6 with\nPaLM 62B and 8B to examine the effect of LM\nscale on LAMBADA . According to the results in\nFigure 6, when we use PaLM 62B, the performance\nof the Goal Decomposition and Sign Agreement\nmodules remain comparable, but the performance\nfor the Fact Check and Rule Selection modules\ndrop substantially\n\nDocument 7:\n. Since identifying the conse-\nquent of each rule is independent of the goal, this\nsub-module only needs to be called once.\n1Note that we select only one fact because the goals\nand sub-goals in the datasets we work with can be\nproved/disproved using single facts; The two modules can\nbe adapted to selected multiple facts if this is not the case.\nAlgorithm 1 LAMBADA\nInput: Theory C = (F, R), Goal G, Max-Depth\nD\n1: factCheckResult = FactCheck(G, F)\n2: if factCheckResult ̸= UNKNOWN then\n3: return\n\nDocument 8:\nproved disproved unknown\nPredicted label\nproved\ndisproved\nunknown\nTrue label\n84\n82\n331\n23 169\n14 180\n53 64\nChain Of Thought\n(a) ProofWriter-PUD (Depth-5)\nproved disproved unknown\nPredicted label\nproved\ndisproved\nunknown\nTrue label\n34\n15\n413\n0 242\n4 257\n19 15\nSelection Inference (b) ProofWriter-PUD (Depth-5)\nproved disproved unknown\nPredicted label\nproved\ndisproved\nunknown\nTrue label\n163\n149\n410\n2 111\n0 127\n14 24\nLAMBADA (c) ProofWriter-PUD (Depth-5)\nproved disproved\nPredicted\n\nDocument 9:\nproof\nby contradiction, disjunction elimination, etc.\n• The calls made to the LM modules in LAMBADA\nare dependent on the value from the previous call.\nThat is, we need to wait for the results from one\ncall before we decide what the next call must be.\nSince making batch calls to the LMs is typically\neasier and faster, future work can find ways to\nimplement LAMBADA with batch LM calls.\n• While we showed that LAMBADA is more effi-\ncient than SI in terms of the number of inference\ncalls it makes to\n\nDocument 10:\n. We show that LAMBADA achieves siz-\nable accuracy boosts over state-of-the-art for-\nward reasoning methods on challenging logical\nreasoning datasets, particularly when deep and\naccurate proof chains are required.\n1 Introduction\nAutomated reasoning, the ability to draw valid con-\nclusions from explicitly provided knowledge, has\nbeen a fundamental goal for AI since its early\ndays (McCarthy, 1959; Hewitt, 1969)\n"
    ],
    "ground_truth": "When LAMBADA predicts PROVED or DISPROVED, the prediction is mostly correct, with slightly higher accuracy for PROVED predictions."
  },
  {
    "question": "According to the document, what happens to the success rate of SI as the inferences progress?",
    "answer": "According to Document 1 and Document 3, the success rate of SI decreases in the later inferences as the size of the input theory becomes larger. Figure 3 in Document 3 visually confirms this trend, showing a decreasing success rate as 'k' (the k-th inference) increases.",
    "contexts": [
      "Document 1:\n. To verify whether the increase in the search\nspace makes forward chaining progressively harder,\nwe measured the success rate of the k-th inference\nof SI for different values of k on Depth-5 of PrOn-\ntoQA (see Appendix B.3 for details). From the\nresults in Figure 3, we can see that the success\nrate indeed decreases in the later inferences of the\nmodel, where the size of the input theory is larger\nand therefore a larger space needs to be searched to\nfind the right combination of facts and rules\n\nDocument 2:\n. The result shows that SI infer-\nences contained no redundant facts only29% of the\ntime; in 7% of the cases, all 5 inferred facts were\nidentical, and in another 10%, only two unique in-\nferences were made\n\nDocument 3:\n1st 2nd 3rd 4th 5th\nk-th inference\n0.30\n0.35\n0.40\n0.45\n0.50Success Rate\n0.53\n0.47\n0.34\n0.31 0.31\nFigure 3: The success rate of the k-th inference of SI\non PrOntoQA (Depth-5) for different values of k. As\nk increases, the size of the input theory becomes larger\nand the success rate decreases.\nthe desired outcome of combining the strengths of\nan LM and a symbolic reasoning algorithm.\nThe results in Figure 2(a) reveal a shortcoming\nof the CoT approach in dealing with UNKNOWN\nlabels\n\nDocument 4:\n. An LM may fail to search this\nspace effectively in a single inference call.\nSI uses an approximation to reduce the search\nspace: it first makes an inference call to an LM to\nselect one fact/rule, then it makes another inference\ncall to select the next fact/rule based on the first\none, and continues to make inference calls until\na halting criterion is met. This approximation re-\n\nDocument 5:\n. SI contains two modules: (1) se-\nlection, which, guided by the goal, selects a subset\nof the facts and rules from which new conclusions\ncan be derived toward proving the goal, and (2)\ninference, which takes the selected facts and rules\nand derives a new conclusion. The two modules\nare called iteratively, each time producing a single\nconclusion that is added back to the theory before\nthe next iteration\n\nDocument 6:\n. the number of facts plus the number of\nrules) is |C|, when making the k-th inference the\nsize of the theory is |C| + k − 1.\nConceptually, as the model produces more in-\nferences, the distance to the goal (in terms of the\nnumber of hops remaining between the goal and\nthe facts) should reduce and so the later inferences\nshould be more accurate\n\nDocument 7:\nDepth-0 Depth-1 Depth-2 Depth-3 Depth-5\nDataset Depth\n101\n102\nAvg. #Inferences\n27.79 27.30\n57.22\n99.45\n219.34\n2.98\n7.26\n10.14 12.77\n18.53\nSI\nLambada\nFigure 7: Comparing LAMBADA and SI w.r.t\n\nDocument 8:\n. the aver-\nage number of inference calls they make per example\nfor different subsets of the ProofWriter-PUD dataset.\nthe correct conclusion and proof (e.g., if in Figure 1\nthe third call toRule Selection only returned Rule5).\nFor Fact Check, when we allow the model to only\nselect one fact, the accuracy is 0.94 but when we\nallow the model to select two facts, the accuracy\nis near perfect\n\nDocument 9:\n. This result is consistent\nwith previous work showing that when LMs are\nasked to solve logical reasoning end-to-end, they\nrely on spurious correlations (Zhang et al., 2022b).\nNote that for modular approaches like SI and LAM-\nBADA , the intermediate modules are impervious to\nthe spurious correlations between the input and the\nlabel and do not suffer from this issue.\n5.3 Forward vs\n\nDocument 10:\n. For running\nSI on ProofWriter-PUD, after obtaining the infer-\nences by running SI, we give the inferences and\nthe goal to our Fact Check module which decides\nif the goal can be proved, disproved, or neither.\nSince ProofWriter-PD and PrOntoQA are binary\ndatasets but LAMBADA makes three-way predic-\ntions (PROVED , DISPROVED , and UNKNOWN ), to\ntest LAMBADA on these datasets, similar to SI we\ncombine the UNKNOWN and DISPROVED predic-\ntions into one class.\nD.1 Datasets for Individual\n"
    ],
    "ground_truth": "The success rate of SI decreases in the later inferences of the model."
  },
  {
    "question": "What are the three main reasons that CoT produces incorrect proof chains, even when the predicted label is correct?",
    "answer": "Based on the provided documents, the three main reasons that Chain-of-Thought (CoT) produces incorrect proof chains, even when the predicted label is correct, are:\n\n1.  **Hallucinating rules or facts:** CoT sometimes relies on rules or facts that are not present in the provided set of rules, and cannot even be derived from them (Document 3, Document 4).\n2.  **Not understanding conjunction:** CoT fails to correctly process and utilize conjunctions in the reasoning process (Document 4).\n3.  **Making invalid derivations:** CoT makes logical errors in the derivation steps, leading to incorrect conclusions (Document 4).",
    "contexts": [
      "Document 1:\nsign of the rules and\nthe sub-goals agree for all cases, except for the very\nfirst rule selected (Rule3) so it correctly concludes\nthat the goal is disproved.\nB.2 Further Analysis of CoT\nIn Figure 2(e), we observed that CoT mostly pro-\nduces wrong proof chains even when the predicted\nlabel is correct\n\nDocument 2:\n. We investigate the reason for this\nbehaviour of CoT in the next section.\n5.2 Proof Accuracy\nTo understand the reason behind the high accuracy\nof CoT on higher depths of ProofWriter-PD, we\nrandomly selected 50 examples from Depth-5 of\nthe dataset where CoT predicted the label correctly,\nand manually verified if the proof chain is correct or\nnot. For comparison, we also manually verified the\nproofs generated by LAMBADA following a similar\nprocedure\n\nDocument 3:\n. Notice that, e.g., in\nthe example with a hallucinated rule, CoT relies\non a rule “if someone chases the mouse then\nthey see the squirrel” which not only does\nnot appear in the provided set of rules, but cannot\neven be derived with a combination of the rules.\nThe high label accuracy of CoT and its low proof\naccuracy on ProofWriter-PD hint at the possibility\nof spurious biases that can be exploited by CoT\n\nDocument 4:\n. Through manually analyzing 50\nexamples for which CoT predicted the correct la-\nbel, we identified three dominant reasons for the\nchains being wrong: 1- hallucinating rules or facts,\n2- not understanding conjunction, and 3- making\ninvalid derivations. In Figure 10, we show failure\nexamples from each category\n\nDocument 5:\n. To answer this\nquestion, we experiment with a backward version\nof CoT where the proofs are written in the back-\nward direction from the goal to the premises. The\nlabel accuracies are presented in Figure 5(a)–(b) for\nProofWriter-PUD and ProofWriter-PD, and their\nproof accuracy on ProofWriter-PD (Depth-5) in\nFigure 5(c). The label accuracy of forward and\nbackward CoT are comparable, but forward CoT\nleads to better performance on PUD and backward\nCoT leads to better performance on PD\n\nDocument 6:\n. The spurious cor-\nrelations also explain the fluctuations in the CoT\nperformance across different depths, as the perfor-\nmance depends on how much those correlations\nappear in the few-shot demonstrations.\nWe reiterate that for SI and LAMBADA , such\nspurious correlations between the input and the\nlabel cannot be exploited because the intermediate\nmodules are impervious to the correlations between\nthe input and the label.\nB.3 Forward Chaining Becomes Progressively\nMore Difficult\nAlgorithms such\n\nDocument 7:\n. The decoding tempera-\nture was set to zero. For testing CoT on PrOntoQA,\nwe used the same demonstration examples as the\noriginal work but slightly changed the wording by\nadding conjunctive words such as “Since” and “So”\nto make the chains have a better flow. The reason\nfor this modification was that we found when work-\ning with PaLM, prompts that have a better flow\nresult in better predictions. This can be viewed\nfrom Figure 13 where we compare the performance\nfor the original prompts vs\n\nDocument 8:\n. Gontier et al. (2020) investigated how\ntransformer models perform when trained to per-\nform forward or backward chaining, and drew con-\nclusions about their internal reasoning strategies.\nWe compare against a popular recent prompting\nstrategy, namely Chain-of-Thought (CoT) (Wei\net al., 2022), from this category.\nVerifiers: To improve CoT, some works train a\nverifier using chain-level labels\n\nDocument 9:\n. Big things are rough. If Erin is nice and Erin is young then Erin is big.\nQuestion: Erin is not nice.\nInference: Since the facts state that Erin is nice [...] Since the facts state that Erin is rough and the rules state that if something is rough then \nit is quiet so we can infer that the statement Erin is not nice is false.\nFigure 10: Examples of wrong CoT proof chains from four different categories\n\nDocument 10:\n. In addition to the few-shot\ndemonstrations in <INPUT>/<LABEL>format in typ-\nical in-context learning settings, in CoT, an in-\ntermediate explanation for the label is also pro-\nvided ( <INPUT>/<EXPLANATION>/<LABEL>). In\nour work, the explanation corresponds to the proof.\nSelection-Inference (SI) (Creswell et al., 2023)\nis a strong modular reasoning approach based on\nforward chaining\n"
    ],
    "ground_truth": "The three dominant reasons for CoT producing wrong proof chains are hallucinating rules or facts, not understanding conjunction, and making invalid derivations."
  },
  {
    "question": "What type of algorithm is LAMBADA?",
    "answer": "Based on the provided documents, LAMBADA is a **recursive, depth-first backward chaining algorithm** designed for automated reasoning in natural language.\n\nHere's why:\n\n*   **Backward Chaining:** Document 6 explicitly states \"LAMBADA: Backward Chaining for Automated Reasoning in Natural Language.\"\n*   **Depth-First:** Document 8 states \"LAMBADA can be understood as a depth-first...\"\n*   **Recursive:** Document 10 mentions \"Since LAMBADA is a recursive algorithm, during the proof of an example Algorithm 1 may be called...\"",
    "contexts": [
      "Document 1:\n. If smaller LMs are\nused, then one may need finer-grained problem de-\ncomposition (e.g., further decomposing the one-to-\nmany comparisons in the selection module). And\nas LMs become larger and stronger in the future,\none could rely on them to solve problems with a\ncoarser-grained decomposition of the problem.\n5.8 Number of Inference Calls\nAnother advantage of LAMBADA is its efficiency\ncompared to other approaches that require multiple\nLM inference calls per example such as SI\n\nDocument 2:\n. Otherwise,\nthe algorithm proceeds with applying rules.\nThe Rule Selection module is used to identify the\nrules Rs from R whose consequent unifies with G.\nOnce the setRs is identified, if LAMBADA can start\nwith the rules that have a higher chance of succeed-\ning at (dis)proving the goal, it can save computa-\ntions and be less error-prone. Therefore, we include\na Rerank function in LAMBADA\n\nDocument 3:\n. The verifier takes\na reasoning chain produced by the model as input\nand judges the quality of the chain (Cobbe et al.,\n2021; Shen et al., 2021; Jhamtani and Clark, 2020;\nZelikman et al., 2022). Using this verifier, one can\nthen generate multiple reasoning chains (e.g., by\nrunning the algorithm multiple times with differ-\nent decoding temperatures) and use the best chain\naccording to the verifier. Since LAMBADA also\n\nDocument 4:\n. This motivates the fourth\nmodule, Sign Agreement, described below.\nGiven a rule r and a goal G, the Sign Agreement\nmodule verifies if the sign of the consequent of r\nagrees or disagrees with the sign of the goal or not.\n3.3 The L AMBADA Algorithm\nAlgorithm 1 provides a high-level description of\nhow the four LM modules described earlier can\nbe integrated with BC to enable text-based logical\nreasoning (the function calls corresponding to LM\nmodules are color-coded).\nLAMBADA can be understood as\n\nDocument 5:\n. We\nshow one such example in Figure 9 (to be discussed\nin Section B).\nLAMBADA may sometimes run into loops.\nFor example, to prove a (sub-)goal “Fiona is\nround?”, after recursively identifying rules that\nunify with it and decomposing it into sub-goals,\nthe algorithm may arrive at a point where it needs\nto prove the “Fiona is round?” sub-goal, which\nis equivalent to the initial goal\n\nDocument 6:\nLAMBADA: Backward Chaining for Automated Reasoning in Natural\nLanguage\nMehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, Deepak Ramachandran\nGoogle Research\n{mehrankazemi, njkim, bhatiad, xxujasime, ramachandrand}@google.com\nAbstract\nRemarkable progress has been made on auto-\nmated reasoning with natural text, by using\nLanguage Models (LMs) and methods such\nas Chain-of-Thought and Selection-Inference.\nThese techniques search for proofs in the for-\nward direction from axioms to the\n\nDocument 7:\n. a lin-\near scan), the Goal Decomposition module breaks\ngoals into sub-goals based on each selected rule\nindependently of the other selected rules, and the\nFact Check module verifies the existence of a fact\nthat entails or contradicts the goal with a linear\nsearch over the facts.\nD Implementation Details\nFor our experiments, we used the PaLM 540B\nmodel (Chowdhery et al., 2022) for all the models\n(both LAMBADA and the baselines) served on a\n4 × 4 TPU v4 architecture\n\nDocument 8:\nare color-coded).\nLAMBADA can be understood as a depth-first\n\nDocument 9:\n. The results show that LAMBADA is robust to lexical and template\nmodifications.\ning modifications for each example: 1- identified\nall entity names and mapped each entity name to\na randomly selected name from the pool, 2- iden-\ntified all animals and mapped each of them to a\nrandomly selected animal from the pool, 3- iden-\ntified all adjectives and mapped each of them to\na randomly selected adjective from the pool, and\n4- identified all verbs and mapped each of them\n(except the to be verbs) to a\n\nDocument 10:\n. In The\nEleventh International Conference on Learning Rep-\nresentations.\nHattie Zhou, Azade Nova, Hugo Larochelle, Aaron\nCourville, Behnam Neyshabur, and Hanie Sedghi.\n2022. Teaching algorithmic reasoning via in-context\nlearning. arXiv:2211.09066.\nA Caching and Avoiding Loops for\nLAMBADA\nSince LAMBADA is a recursive algorithm, during\nthe proof of an example Algorithm 1 may be called\n"
    ],
    "ground_truth": "LAMBADA is a recursive algorithm."
  }
]